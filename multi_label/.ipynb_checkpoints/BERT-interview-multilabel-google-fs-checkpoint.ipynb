{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\bert\\optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from bert import modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##use downloaded model, change path accordingly\n",
    "BERT_VOCAB= './uncased_L-12_H-768_A-12/vocab.txt'\n",
    "BERT_INIT_CHKPNT = './uncased_L-12_H-768_A-12/bert_model.ckpt'\n",
    "BERT_CONFIG = './uncased_L-12_H-768_A-12/bert_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenization.validate_case_matches_checkpoint(True,BERT_INIT_CHKPNT)\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=BERT_VOCAB, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unnamed:_0</th>\n",
       "      <th>interview_id</th>\n",
       "      <th>interview_date</th>\n",
       "      <th>record_creation_date</th>\n",
       "      <th>venue</th>\n",
       "      <th>address_name</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state_/_province</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>...</th>\n",
       "      <th>interviewer_relationship_5_to_storyteller_2</th>\n",
       "      <th>language_1</th>\n",
       "      <th>language_2</th>\n",
       "      <th>language_3</th>\n",
       "      <th>keywords_-_fixed_subjects</th>\n",
       "      <th>keywords_-_general</th>\n",
       "      <th>keywords_-_places</th>\n",
       "      <th>deep_speech</th>\n",
       "      <th>human_transcript</th>\n",
       "      <th>google_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>MBY005779</td>\n",
       "      <td>10/2/09 11:30</td>\n",
       "      <td>10/2/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Penrose Public Library</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>80911.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Achievements and Awards\\n\\nArchitecture\\n\\nChi...</td>\n",
       "      <td>1941 Chevy\\n\\n1960 Starline\\n\\nagriculture\\n\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i my name is lisande i am sixty eight years ol...</td>\n",
       "      <td>LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...</td>\n",
       "      <td>my name is Teresa Sanchez I am 68 years old to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>MBY005843</td>\n",
       "      <td>10/14/09 11:30</td>\n",
       "      <td>10/14/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Penrose Public Library</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>80911.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Achievements and Awards\\n\\nChildren\\n\\nChristm...</td>\n",
       "      <td>Air Corps\\n\\ncarols\\n\\nchurch\\n\\nColorado Spri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my name is simply a le and shut stenia i go by...</td>\n",
       "      <td>Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...</td>\n",
       "      <td>my name is Cynthia Lee and shoots tehnika I go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>MBY005844</td>\n",
       "      <td>10/14/09 12:30</td>\n",
       "      <td>10/14/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Penrose Public Library</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>80911.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Army\\n\\nBest Friends\\n\\nBirth\\n\\nChanges In Ed...</td>\n",
       "      <td>Angela Gonzales\\n\\nappearance\\n\\nbirth of firs...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>okay i'm tones rein twenty nine in today's oct...</td>\n",
       "      <td>Denise Ricks: [00:00:00] Okay. I'm Denise Rick...</td>\n",
       "      <td>okay I'm Denise Rick's I'm 29 today is October...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>MBY005884</td>\n",
       "      <td>10/25/09 12:30</td>\n",
       "      <td>10/25/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>204 South Main Street</td>\n",
       "      <td>Wichita</td>\n",
       "      <td>KS</td>\n",
       "      <td>67202.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Achievements and Awards\\n\\nBirth\\n\\nChanges In...</td>\n",
       "      <td>AFLCIO\\n\\nBarack Obama\\n\\nBlind Rights\\n\\nbrai...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i mane is highly christine johnson i'm thirty ...</td>\n",
       "      <td>Heidi Christine Johnson: [00:00:00] Hi. My nam...</td>\n",
       "      <td>hi my name is Heidi Christine Johnson I'm 38 y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>MBY005906</td>\n",
       "      <td>10/31/09 9:30</td>\n",
       "      <td>10/31/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>204 South Main Street</td>\n",
       "      <td>Wichita</td>\n",
       "      <td>KS</td>\n",
       "      <td>67202.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Birth\\n\\nBosses\\n\\nComing Of Age\\n\\nCommunity ...</td>\n",
       "      <td>cohorts (groups of friends)\\n\\ncollege\\n\\ncraf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i am the morning my name's carcava i am twenty...</td>\n",
       "      <td>Sarah Culver: [00:00:00] Hi. Good morning. My ...</td>\n",
       "      <td>hi good morning my name is Sarah Culver I'm 24...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unnamed:_0 interview_id  interview_date record_creation_date  \\\n",
       "9            9    MBY005779   10/2/09 11:30              10/2/09   \n",
       "16          16    MBY005843  10/14/09 11:30             10/14/09   \n",
       "17          17    MBY005844  10/14/09 12:30             10/14/09   \n",
       "22          22    MBY005884  10/25/09 12:30             10/25/09   \n",
       "23          23    MBY005906   10/31/09 9:30             10/31/09   \n",
       "\n",
       "                     venue      address_name                  street  \\\n",
       "9   MobileBooth West (MBY)  MobileBooth West  Penrose Public Library   \n",
       "16  MobileBooth West (MBY)  MobileBooth West  Penrose Public Library   \n",
       "17  MobileBooth West (MBY)  MobileBooth West  Penrose Public Library   \n",
       "22  MobileBooth West (MBY)  MobileBooth West   204 South Main Street   \n",
       "23  MobileBooth West (MBY)  MobileBooth West   204 South Main Street   \n",
       "\n",
       "                city state_/_province  postal_code  ...  \\\n",
       "9   Colorado Springs               CO      80911.0  ...   \n",
       "16  Colorado Springs               CO      80911.0  ...   \n",
       "17  Colorado Springs               CO      80911.0  ...   \n",
       "22           Wichita               KS      67202.0  ...   \n",
       "23           Wichita               KS      67202.0  ...   \n",
       "\n",
       "   interviewer_relationship_5_to_storyteller_2 language_1 language_2  \\\n",
       "9                                          NaN    English        NaN   \n",
       "16                                         NaN    English        NaN   \n",
       "17                                         NaN    English        NaN   \n",
       "22                                         NaN    English        NaN   \n",
       "23                                         NaN    English        NaN   \n",
       "\n",
       "   language_3                          keywords_-_fixed_subjects  \\\n",
       "9         NaN  Achievements and Awards\\n\\nArchitecture\\n\\nChi...   \n",
       "16        NaN  Achievements and Awards\\n\\nChildren\\n\\nChristm...   \n",
       "17        NaN  Army\\n\\nBest Friends\\n\\nBirth\\n\\nChanges In Ed...   \n",
       "22        NaN  Achievements and Awards\\n\\nBirth\\n\\nChanges In...   \n",
       "23        NaN  Birth\\n\\nBosses\\n\\nComing Of Age\\n\\nCommunity ...   \n",
       "\n",
       "                                   keywords_-_general keywords_-_places  \\\n",
       "9   1941 Chevy\\n\\n1960 Starline\\n\\nagriculture\\n\\n...               NaN   \n",
       "16  Air Corps\\n\\ncarols\\n\\nchurch\\n\\nColorado Spri...               NaN   \n",
       "17  Angela Gonzales\\n\\nappearance\\n\\nbirth of firs...               NaN   \n",
       "22  AFLCIO\\n\\nBarack Obama\\n\\nBlind Rights\\n\\nbrai...               NaN   \n",
       "23  cohorts (groups of friends)\\n\\ncollege\\n\\ncraf...               NaN   \n",
       "\n",
       "                                          deep_speech  \\\n",
       "9   i my name is lisande i am sixty eight years ol...   \n",
       "16  my name is simply a le and shut stenia i go by...   \n",
       "17  okay i'm tones rein twenty nine in today's oct...   \n",
       "22  i mane is highly christine johnson i'm thirty ...   \n",
       "23  i am the morning my name's carcava i am twenty...   \n",
       "\n",
       "                                     human_transcript  \\\n",
       "9   LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...   \n",
       "16  Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...   \n",
       "17  Denise Ricks: [00:00:00] Okay. I'm Denise Rick...   \n",
       "22  Heidi Christine Johnson: [00:00:00] Hi. My nam...   \n",
       "23  Sarah Culver: [00:00:00] Hi. Good morning. My ...   \n",
       "\n",
       "                                    google_transcript  \n",
       "9   my name is Teresa Sanchez I am 68 years old to...  \n",
       "16  my name is Cynthia Lee and shoots tehnika I go...  \n",
       "17  okay I'm Denise Rick's I'm 29 today is October...  \n",
       "22  hi my name is Heidi Christine Johnson I'm 38 y...  \n",
       "23  hi good morning my name is Sarah Culver I'm 24...  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_raw = pd.read_csv('../Data/transcript.csv')\n",
    "transcript_raw.columns = transcript_raw.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "transcript_raw = transcript_raw[transcript_raw.human_transcript.notnull()]\n",
    "transcript_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_raw = pd.read_csv('../Data/labels.csv')\n",
    "label_raw.columns = label_raw.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "# label_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unnamed:_0</th>\n",
       "      <th>interview_id</th>\n",
       "      <th>interview_date_x</th>\n",
       "      <th>record_creation_date_x</th>\n",
       "      <th>venue_x</th>\n",
       "      <th>address_name_x</th>\n",
       "      <th>street_x</th>\n",
       "      <th>city_x</th>\n",
       "      <th>state_/_province_x</th>\n",
       "      <th>postal_code_x</th>\n",
       "      <th>...</th>\n",
       "      <th>interviewer_relationship_1_to_storyteller_1_y</th>\n",
       "      <th>language_1_y</th>\n",
       "      <th>keywords_-_fixed_subjects_y</th>\n",
       "      <th>keywords_-_general_y</th>\n",
       "      <th>label1-fs</th>\n",
       "      <th>label2-fs</th>\n",
       "      <th>label3-fs</th>\n",
       "      <th>label1-g</th>\n",
       "      <th>label2-g</th>\n",
       "      <th>label3-g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>MBY005779</td>\n",
       "      <td>10/2/09 11:30</td>\n",
       "      <td>10/2/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Penrose Public Library</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>80911.0</td>\n",
       "      <td>...</td>\n",
       "      <td>husband</td>\n",
       "      <td>English</td>\n",
       "      <td>Achievements and Awards\\n\\nArchitecture\\n\\nChi...</td>\n",
       "      <td>1941 Chevy\\n\\n1960 Starline\\n\\nagriculture\\n\\n...</td>\n",
       "      <td>Parents</td>\n",
       "      <td>Schools</td>\n",
       "      <td>Children</td>\n",
       "      <td>memories of growing up</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>anecdotes (humorous but true stories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>MBY005843</td>\n",
       "      <td>10/14/09 11:30</td>\n",
       "      <td>10/14/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Penrose Public Library</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>80911.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mother</td>\n",
       "      <td>English</td>\n",
       "      <td>Achievements and Awards\\n\\nChildren\\n\\nChristm...</td>\n",
       "      <td>Air Corps\\n\\ncarols\\n\\nchurch\\n\\nColorado Spri...</td>\n",
       "      <td>Parents</td>\n",
       "      <td>Children</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>memories of growing up</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>memories of former times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>MBY005844</td>\n",
       "      <td>10/14/09 12:30</td>\n",
       "      <td>10/14/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Penrose Public Library</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>80911.0</td>\n",
       "      <td>...</td>\n",
       "      <td>father</td>\n",
       "      <td>English</td>\n",
       "      <td>Army\\n\\nBest Friends\\n\\nBirth\\n\\nChanges In Ed...</td>\n",
       "      <td>Angela Gonzales\\n\\nappearance\\n\\nbirth of firs...</td>\n",
       "      <td>Parents</td>\n",
       "      <td>Children</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>memories of growing up</td>\n",
       "      <td>social beliefs and practices</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>MBY005884</td>\n",
       "      <td>10/25/09 12:30</td>\n",
       "      <td>10/25/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>204 South Main Street</td>\n",
       "      <td>Wichita</td>\n",
       "      <td>KS</td>\n",
       "      <td>67202.0</td>\n",
       "      <td>...</td>\n",
       "      <td>friend</td>\n",
       "      <td>English</td>\n",
       "      <td>Achievements and Awards\\n\\nBirth\\n\\nChanges In...</td>\n",
       "      <td>AFLCIO\\n\\nBarack Obama\\n\\nBlind Rights\\n\\nbrai...</td>\n",
       "      <td>Schools</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>Immigration Stories</td>\n",
       "      <td>memories of growing up</td>\n",
       "      <td>social beliefs and practices</td>\n",
       "      <td>memories of former times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>MBY005906</td>\n",
       "      <td>10/31/09 9:30</td>\n",
       "      <td>10/31/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>204 South Main Street</td>\n",
       "      <td>Wichita</td>\n",
       "      <td>KS</td>\n",
       "      <td>67202.0</td>\n",
       "      <td>...</td>\n",
       "      <td>interviewee</td>\n",
       "      <td>English</td>\n",
       "      <td>Birth\\n\\nBosses\\n\\nComing Of Age\\n\\nCommunity ...</td>\n",
       "      <td>cohorts (groups of friends)\\n\\ncollege\\n\\ncraf...</td>\n",
       "      <td>Parents</td>\n",
       "      <td>Schools</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>memories of growing up</td>\n",
       "      <td>social beliefs and practices</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unnamed:_0 interview_id interview_date_x record_creation_date_x  \\\n",
       "0           9    MBY005779    10/2/09 11:30                10/2/09   \n",
       "1          16    MBY005843   10/14/09 11:30               10/14/09   \n",
       "2          17    MBY005844   10/14/09 12:30               10/14/09   \n",
       "3          22    MBY005884   10/25/09 12:30               10/25/09   \n",
       "4          23    MBY005906    10/31/09 9:30               10/31/09   \n",
       "\n",
       "                  venue_x    address_name_x                street_x  \\\n",
       "0  MobileBooth West (MBY)  MobileBooth West  Penrose Public Library   \n",
       "1  MobileBooth West (MBY)  MobileBooth West  Penrose Public Library   \n",
       "2  MobileBooth West (MBY)  MobileBooth West  Penrose Public Library   \n",
       "3  MobileBooth West (MBY)  MobileBooth West   204 South Main Street   \n",
       "4  MobileBooth West (MBY)  MobileBooth West   204 South Main Street   \n",
       "\n",
       "             city_x state_/_province_x  postal_code_x  ...  \\\n",
       "0  Colorado Springs                 CO        80911.0  ...   \n",
       "1  Colorado Springs                 CO        80911.0  ...   \n",
       "2  Colorado Springs                 CO        80911.0  ...   \n",
       "3           Wichita                 KS        67202.0  ...   \n",
       "4           Wichita                 KS        67202.0  ...   \n",
       "\n",
       "  interviewer_relationship_1_to_storyteller_1_y language_1_y  \\\n",
       "0                                       husband      English   \n",
       "1                                        mother      English   \n",
       "2                                        father      English   \n",
       "3                                        friend      English   \n",
       "4                                   interviewee      English   \n",
       "\n",
       "                         keywords_-_fixed_subjects_y  \\\n",
       "0  Achievements and Awards\\n\\nArchitecture\\n\\nChi...   \n",
       "1  Achievements and Awards\\n\\nChildren\\n\\nChristm...   \n",
       "2  Army\\n\\nBest Friends\\n\\nBirth\\n\\nChanges In Ed...   \n",
       "3  Achievements and Awards\\n\\nBirth\\n\\nChanges In...   \n",
       "4  Birth\\n\\nBosses\\n\\nComing Of Age\\n\\nCommunity ...   \n",
       "\n",
       "                                keywords_-_general_y label1-fs     label2-fs  \\\n",
       "0  1941 Chevy\\n\\n1960 Starline\\n\\nagriculture\\n\\n...   Parents       Schools   \n",
       "1  Air Corps\\n\\ncarols\\n\\nchurch\\n\\nColorado Spri...   Parents      Children   \n",
       "2  Angela Gonzales\\n\\nappearance\\n\\nbirth of firs...   Parents      Children   \n",
       "3  AFLCIO\\n\\nBarack Obama\\n\\nBlind Rights\\n\\nbrai...   Schools  Workday Life   \n",
       "4  cohorts (groups of friends)\\n\\ncollege\\n\\ncraf...   Parents       Schools   \n",
       "\n",
       "             label3-fs                label1-g                      label2-g  \\\n",
       "0             Children  memories of growing up                     ethnicity   \n",
       "1         Workday Life  memories of growing up                     ethnicity   \n",
       "2         Workday Life  memories of growing up  social beliefs and practices   \n",
       "3  Immigration Stories  memories of growing up  social beliefs and practices   \n",
       "4         Workday Life  memories of growing up  social beliefs and practices   \n",
       "\n",
       "                                label3-g  \n",
       "0  anecdotes (humorous but true stories)  \n",
       "1               memories of former times  \n",
       "2                              ethnicity  \n",
       "3               memories of former times  \n",
       "4                              ethnicity  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw = transcript_raw.merge(label_raw, on='interview_id')\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_transcript</th>\n",
       "      <th>label1-fs</th>\n",
       "      <th>label2-fs</th>\n",
       "      <th>label3-fs</th>\n",
       "      <th>interview_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...</td>\n",
       "      <td>Parents</td>\n",
       "      <td>Schools</td>\n",
       "      <td>Children</td>\n",
       "      <td>MBY005779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...</td>\n",
       "      <td>Parents</td>\n",
       "      <td>Children</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>MBY005843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Denise Ricks: [00:00:00] Okay. I'm Denise Rick...</td>\n",
       "      <td>Parents</td>\n",
       "      <td>Children</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>MBY005844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heidi Christine Johnson: [00:00:00] Hi. My nam...</td>\n",
       "      <td>Schools</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>Immigration Stories</td>\n",
       "      <td>MBY005884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sarah Culver: [00:00:00] Hi. Good morning. My ...</td>\n",
       "      <td>Parents</td>\n",
       "      <td>Schools</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>MBY005906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    human_transcript label1-fs     label2-fs  \\\n",
       "0  LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...   Parents       Schools   \n",
       "1  Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...   Parents      Children   \n",
       "2  Denise Ricks: [00:00:00] Okay. I'm Denise Rick...   Parents      Children   \n",
       "3  Heidi Christine Johnson: [00:00:00] Hi. My nam...   Schools  Workday Life   \n",
       "4  Sarah Culver: [00:00:00] Hi. Good morning. My ...   Parents       Schools   \n",
       "\n",
       "             label3-fs interview_id  \n",
       "0             Children    MBY005779  \n",
       "1         Workday Life    MBY005843  \n",
       "2         Workday Life    MBY005844  \n",
       "3  Immigration Stories    MBY005884  \n",
       "4         Workday Life    MBY005906  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw = train_raw[['human_transcript', 'label1-fs', 'label2-fs', 'label3-fs', 'interview_id']]\n",
    "train_raw.reset_index(inplace=True, drop=True)\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(train_raw[['label1-fs', 'label2-fs', 'label3-fs']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_transcript</th>\n",
       "      <th>labels</th>\n",
       "      <th>interview_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...</td>\n",
       "      <td>Parents,Schools,Children</td>\n",
       "      <td>MBY005779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...</td>\n",
       "      <td>Parents,Children,Workday Life</td>\n",
       "      <td>MBY005843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Denise Ricks: [00:00:00] Okay. I'm Denise Rick...</td>\n",
       "      <td>Parents,Children,Workday Life</td>\n",
       "      <td>MBY005844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heidi Christine Johnson: [00:00:00] Hi. My nam...</td>\n",
       "      <td>Schools,Workday Life,Immigration Stories</td>\n",
       "      <td>MBY005884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sarah Culver: [00:00:00] Hi. Good morning. My ...</td>\n",
       "      <td>Parents,Schools,Workday Life</td>\n",
       "      <td>MBY005906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    human_transcript  \\\n",
       "0  LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...   \n",
       "1  Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...   \n",
       "2  Denise Ricks: [00:00:00] Okay. I'm Denise Rick...   \n",
       "3  Heidi Christine Johnson: [00:00:00] Hi. My nam...   \n",
       "4  Sarah Culver: [00:00:00] Hi. Good morning. My ...   \n",
       "\n",
       "                                     labels interview_id  \n",
       "0                  Parents,Schools,Children    MBY005779  \n",
       "1             Parents,Children,Workday Life    MBY005843  \n",
       "2             Parents,Children,Workday Life    MBY005844  \n",
       "3  Schools,Workday Life,Immigration Stories    MBY005884  \n",
       "4              Parents,Schools,Workday Life    MBY005906  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw['labels'] = train_raw[train_raw.columns[1:4]].apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "train_raw = train_raw[['human_transcript', 'labels', 'interview_id']]\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Children</th>\n",
       "      <th>Identity</th>\n",
       "      <th>Immigration Stories</th>\n",
       "      <th>Job Satisfaction</th>\n",
       "      <th>Marriage</th>\n",
       "      <th>Parents</th>\n",
       "      <th>Schools</th>\n",
       "      <th>Siblings</th>\n",
       "      <th>Teachers</th>\n",
       "      <th>Town Life</th>\n",
       "      <th>Workday Life</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Children  Identity  Immigration Stories  Job Satisfaction  Marriage  \\\n",
       "0         1         0                    0                 0         0   \n",
       "1         1         0                    0                 0         0   \n",
       "2         1         0                    0                 0         0   \n",
       "3         0         0                    1                 0         0   \n",
       "4         0         0                    0                 0         0   \n",
       "\n",
       "   Parents  Schools  Siblings  Teachers  Town Life  Workday Life  \n",
       "0        1        1         0         0          0             0  \n",
       "1        1        0         0         0          0             1  \n",
       "2        1        0         0         0          0             1  \n",
       "3        0        1         0         0          0             1  \n",
       "4        1        1         0         0          0             1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned = train_raw.labels.str.split(',', expand=True).stack()\n",
    "label_df = pd.get_dummies(cleaned).groupby(level=0).sum()\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interview_id</th>\n",
       "      <th>human_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MBY005779</td>\n",
       "      <td>LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MBY005843</td>\n",
       "      <td>Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MBY005844</td>\n",
       "      <td>Denise Ricks: [00:00:00] Okay. I'm Denise Rick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MBY005884</td>\n",
       "      <td>Heidi Christine Johnson: [00:00:00] Hi. My nam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MBY005906</td>\n",
       "      <td>Sarah Culver: [00:00:00] Hi. Good morning. My ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  interview_id                                   human_transcript\n",
       "0    MBY005779  LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...\n",
       "1    MBY005843  Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...\n",
       "2    MBY005844  Denise Ricks: [00:00:00] Okay. I'm Denise Rick...\n",
       "3    MBY005884  Heidi Christine Johnson: [00:00:00] Hi. My nam...\n",
       "4    MBY005906  Sarah Culver: [00:00:00] Hi. Good morning. My ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript = train_raw[['interview_id', 'human_transcript']]\n",
    "# train_raw = pd.concat(train_raw[['human_transcript']], pd.get_dummies(cleaned).groupby(level=0).sum())\n",
    "transcript.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interview_id</th>\n",
       "      <th>human_transcript</th>\n",
       "      <th>children</th>\n",
       "      <th>identity</th>\n",
       "      <th>immigration_stories</th>\n",
       "      <th>job_satisfaction</th>\n",
       "      <th>marriage</th>\n",
       "      <th>parents</th>\n",
       "      <th>schools</th>\n",
       "      <th>siblings</th>\n",
       "      <th>teachers</th>\n",
       "      <th>town_life</th>\n",
       "      <th>workday_life</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MBY005779</td>\n",
       "      <td>LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MBY005843</td>\n",
       "      <td>Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MBY005844</td>\n",
       "      <td>Denise Ricks: [00:00:00] Okay. I'm Denise Rick...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MBY005884</td>\n",
       "      <td>Heidi Christine Johnson: [00:00:00] Hi. My nam...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MBY005906</td>\n",
       "      <td>Sarah Culver: [00:00:00] Hi. Good morning. My ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  interview_id                                   human_transcript  children  \\\n",
       "0    MBY005779  LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...         1   \n",
       "1    MBY005843  Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...         1   \n",
       "2    MBY005844  Denise Ricks: [00:00:00] Okay. I'm Denise Rick...         1   \n",
       "3    MBY005884  Heidi Christine Johnson: [00:00:00] Hi. My nam...         0   \n",
       "4    MBY005906  Sarah Culver: [00:00:00] Hi. Good morning. My ...         0   \n",
       "\n",
       "   identity  immigration_stories  job_satisfaction  marriage  parents  \\\n",
       "0         0                    0                 0         0        1   \n",
       "1         0                    0                 0         0        1   \n",
       "2         0                    0                 0         0        1   \n",
       "3         0                    1                 0         0        0   \n",
       "4         0                    0                 0         0        1   \n",
       "\n",
       "   schools  siblings  teachers  town_life  workday_life  \n",
       "0        1         0         0          0             0  \n",
       "1        0         0         0          0             1  \n",
       "2        0         0         0          0             1  \n",
       "3        1         0         0          0             1  \n",
       "4        1         0         0          0             1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw = pd.concat([transcript, label_df], axis=1)\n",
    "train_raw.columns = train_raw.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 13)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(train_raw, test_size=0.2)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 'interview_id'\n",
    "DATA_COLUMN = 'human_transcript'\n",
    "LABEL_COLUMNS = ['children', 'identity', 'immigration_stories', 'job_satisfaction', 'marriage', 'parents', 'schools', 'siblings', 'teachers', 'town_life', 'workday_life']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            labels: (Optional) [string]. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids, is_real_example=True):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids,\n",
    "        self.is_real_example=is_real_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_examples(df, labels_available=True):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, row) in enumerate(df.values):\n",
    "        guid = row[0]\n",
    "        text_a = row[1]\n",
    "        if labels_available:\n",
    "            labels = row[2:]\n",
    "        else:\n",
    "            labels = [0] * 11\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=text_a, labels=labels))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VAL_RATIO = 0.9\n",
    "LEN = train_raw.shape[0]\n",
    "SIZE_TRAIN = int(TRAIN_VAL_RATIO*LEN)\n",
    "\n",
    "x_train = train_raw[:SIZE_TRAIN]\n",
    "x_val = train_raw[SIZE_TRAIN:]\n",
    "\n",
    "train_examples = create_examples(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples,  max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        print(example.text_a)\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        \n",
    "        labels_ids = []\n",
    "        for label in example.labels:\n",
    "            labels_ids.append(int(label))\n",
    "\n",
    "        if ex_index < 0:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %s)\" % (example.labels, labels_ids))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_ids=labels_ids))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 5.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 1000\n",
    "SAVE_SUMMARY_STEPS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "    When running eval/predict on the TPU, we need to pad the number of examples\n",
    "    to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "    size. The alternative is to drop the last batch, which is bad because it means\n",
    "    the entire output data won't be generated.\n",
    "    We use this class instead of `None` because treating `None` as padding\n",
    "    battches could cause silent errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "def convert_single_example(ex_index, example, max_seq_length,\n",
    "                           tokenizer):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        return InputFeatures(\n",
    "            input_ids=[0] * max_seq_length,\n",
    "            input_mask=[0] * max_seq_length,\n",
    "            segment_ids=[0] * max_seq_length,\n",
    "            label_ids=0,\n",
    "            is_real_example=False)\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    if tokens_b:\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0     0   0   0  0     0 0\n",
    "    # \n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    if tokens_b:\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    labels_ids = []\n",
    "    for label in example.labels:\n",
    "        labels_ids.append(int(label))\n",
    "\n",
    "\n",
    "    feature = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_ids=labels_ids,\n",
    "        is_real_example=True)\n",
    "    return feature\n",
    "\n",
    "\n",
    "def file_based_convert_examples_to_features(\n",
    "        examples, max_seq_length, tokenizer, output_file):\n",
    "    \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
    "\n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        #if ex_index % 10000 == 0:\n",
    "            #tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        feature = convert_single_example(ex_index, example,\n",
    "                                         max_seq_length, tokenizer)\n",
    "\n",
    "        def create_int_feature(values):\n",
    "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "            return f\n",
    "\n",
    "        features = collections.OrderedDict()\n",
    "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "        features[\"is_real_example\"] = create_int_feature(\n",
    "            [int(feature.is_real_example)])\n",
    "        if isinstance(feature.label_ids, list):\n",
    "            label_ids = feature.label_ids\n",
    "        else:\n",
    "            label_ids = feature.label_ids[0]\n",
    "        features[\"label_ids\"] = create_int_feature(label_ids)\n",
    "\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def file_based_input_fn_builder(input_file, seq_length, is_training,\n",
    "                                drop_remainder):\n",
    "    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "    name_to_features = {\n",
    "        \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"label_ids\": tf.FixedLenFeature([11], tf.int64),\n",
    "        \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    def _decode_record(record, name_to_features):\n",
    "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "        example = tf.parse_single_example(record, name_to_features)\n",
    "\n",
    "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "        # So cast all int64 to int32.\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.to_int32(t)\n",
    "            example[name] = t\n",
    "\n",
    "        return example\n",
    "\n",
    "    def input_fn(params):\n",
    "        \"\"\"The actual input function.\"\"\"\n",
    "        batch_size = params[\"batch_size\"]\n",
    "\n",
    "        # For training, we want a lot of parallel reading and shuffling.\n",
    "        # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "        d = tf.data.TFRecordDataset(input_file)\n",
    "        if is_training:\n",
    "            d = d.repeat()\n",
    "            d = d.shuffle(buffer_size=100)\n",
    "\n",
    "        d = d.apply(\n",
    "            tf.contrib.data.map_and_batch(\n",
    "                lambda record: _decode_record(record, name_to_features),\n",
    "                batch_size=batch_size,\n",
    "                drop_remainder=drop_remainder))\n",
    "\n",
    "        return d\n",
    "\n",
    "    return input_fn\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_examples) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file = './human/train.tf_record'\n",
    "train_file =  os.path.join(\"./human/fs\", 'train.tf_record').replace(\"\\\\\",\"/\")\n",
    "#filename = Path(train_file)\n",
    "if not os.path.exists(train_file):\n",
    "    open(train_file, 'w').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Running training *****\n",
      "INFO:tensorflow:  Num examples = 51\n",
      "INFO:tensorflow:  Batch size = 16\n",
      "INFO:tensorflow:  Num steps = 15\n"
     ]
    }
   ],
   "source": [
    "file_based_convert_examples_to_features(\n",
    "            train_examples, MAX_SEQ_LENGTH, tokenizer, train_file)\n",
    "tf.logging.info(\"***** Running training *****\")\n",
    "tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
    "tf.logging.info(\"  Batch size = %d\", BATCH_SIZE)\n",
    "tf.logging.info(\"  Num steps = %d\", num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = file_based_input_fn_builder(\n",
    "    input_file=train_file,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
    "                 labels, num_labels, use_one_hot_embeddings):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "    model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "    # In the demo, we are doing a simple classification task on the entire\n",
    "    # segment.\n",
    "    #\n",
    "    # If you want to use the token-level output, use model.get_sequence_output()\n",
    "    # instead.\n",
    "    output_layer = model.get_pooled_output()\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        if is_training:\n",
    "            # I.e., 0.1 dropout\n",
    "            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        \n",
    "        # probabilities = tf.nn.softmax(logits, axis=-1) ### multiclass case\n",
    "        probabilities = tf.nn.sigmoid(logits)#### multi-label case\n",
    "        \n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "        tf.logging.info(\"num_labels:{};logits:{};labels:{}\".format(num_labels, logits, labels))\n",
    "        per_example_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "\n",
    "        # probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "        # log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        #\n",
    "        # one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "        #\n",
    "        # per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        # loss = tf.reduce_mean(per_example_loss)\n",
    "\n",
    "        return (loss, per_example_loss, logits, probabilities)\n",
    "\n",
    "\n",
    "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "        #tf.logging.info(\"*** Features ***\")\n",
    "        #for name in sorted(features.keys()):\n",
    "        #    tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "        is_real_example = None\n",
    "        if \"is_real_example\" in features:\n",
    "             is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
    "        else:\n",
    "             is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
    "\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "        (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
    "            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
    "            num_labels, use_one_hot_embeddings)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        initialized_variable_names = {}\n",
    "        scaffold_fn = None\n",
    "        if init_checkpoint:\n",
    "            (assignment_map, initialized_variable_names\n",
    "             ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "            if use_tpu:\n",
    "\n",
    "                def tpu_scaffold():\n",
    "                    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "                    return tf.train.Scaffold()\n",
    "\n",
    "                scaffold_fn = tpu_scaffold\n",
    "            else:\n",
    "                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "        tf.logging.info(\"**** Trainable Variables ****\")\n",
    "        for var in tvars:\n",
    "            init_string = \"\"\n",
    "            if var.name in initialized_variable_names:\n",
    "                init_string = \", *INIT_FROM_CKPT*\"\n",
    "            #tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,init_string)\n",
    "\n",
    "        output_spec = None\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\n",
    "            train_op = optimization.create_optimizer(\n",
    "                total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
    "\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                train_op=train_op,\n",
    "                scaffold=scaffold_fn)\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\n",
    "            def metric_fn(per_example_loss, label_ids, probabilities, is_real_example):\n",
    "\n",
    "                logits_split = tf.split(probabilities, num_labels, axis=-1)\n",
    "                label_ids_split = tf.split(label_ids, num_labels, axis=-1)\n",
    "                # metrics change to auc of every class\n",
    "                eval_dict = {}\n",
    "                for j, logits in enumerate(logits_split):\n",
    "                    label_id_ = tf.cast(label_ids_split[j], dtype=tf.int32)\n",
    "                    current_auc, update_op_auc = tf.metrics.auc(label_id_, logits)\n",
    "                    eval_dict[str(j)] = (current_auc, update_op_auc)\n",
    "                eval_dict['eval_loss'] = tf.metrics.mean(values=per_example_loss)\n",
    "                return eval_dict\n",
    "\n",
    "                ## original eval metrics\n",
    "                # predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "                # accuracy = tf.metrics.accuracy(\n",
    "                #     labels=label_ids, predictions=predictions, weights=is_real_example)\n",
    "                # loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
    "                # return {\n",
    "                #     \"eval_accuracy\": accuracy,\n",
    "                #     \"eval_loss\": loss,\n",
    "                # }\n",
    "\n",
    "            eval_metrics = metric_fn(per_example_loss, label_ids, probabilities, is_real_example)\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                eval_metric_ops=eval_metrics,\n",
    "                scaffold=scaffold_fn)\n",
    "        else:\n",
    "            print(\"mode:\", mode,\"probabilities:\", probabilities)\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions={\"probabilities\": probabilities},\n",
    "                scaffold=scaffold_fn)\n",
    "        return output_spec\n",
    "\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./human/fs/output\"\n",
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    keep_checkpoint_max=1,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './human/fs/output', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000025259FB65C0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n",
    "model_fn = model_fn_builder(\n",
    "  bert_config=bert_config,\n",
    "  num_labels= len(LABEL_COLUMNS),\n",
    "  init_checkpoint=BERT_INIT_CHKPNT,\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps,\n",
    "  use_tpu=False,\n",
    "  use_one_hot_embeddings=False)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow_core\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-24-82a8abc2eacc>:179: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow_core\\contrib\\data\\python\\ops\\batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow_core\\python\\autograph\\converters\\directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-24-82a8abc2eacc>:159: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\bert\\modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\bert\\modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\bert\\modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\bert\\modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\bert\\modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "INFO:tensorflow:num_labels:11;logits:Tensor(\"loss/BiasAdd:0\", shape=(16, 11), dtype=float32);labels:Tensor(\"loss/Cast:0\", shape=(16, 11), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\bert\\optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\bert\\optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./human/fs/output\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.7542591, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 15 into ./human/fs/output\\model.ckpt.\n",
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Loss for final step: 0.33547032.\n",
      "Training took time  0:01:24.590136\n"
     ]
    }
   ],
   "source": [
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file = os.path.join('./human/fs', \"eval.tf_record\")\n",
    "#filename = Path(train_file)\n",
    "if not os.path.exists(eval_file):\n",
    "    open(eval_file, 'w').close()\n",
    "\n",
    "eval_examples = create_examples(x_val)\n",
    "file_based_convert_examples_to_features(\n",
    "    eval_examples, MAX_SEQ_LENGTH, tokenizer, eval_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:num_labels:11;logits:Tensor(\"loss/BiasAdd:0\", shape=(?, 11), dtype=float32);labels:Tensor(\"loss/Cast:0\", shape=(?, 11), dtype=float32)\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow_core\\python\\ops\\metrics_impl.py:808: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-09-24T02:21:12Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./human/fs/output\\model.ckpt-15\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2020-09-24-02:21:15\n",
      "INFO:tensorflow:Saving dict for global step 15: 0 = 0.49999988, 1 = 0.50000036, 10 = 0.3750002, 2 = 0.20000075, 3 = 0.99999976, 4 = 0.9999998, 5 = 1.9999977e-07, 6 = 0.49999988, 7 = 0.9999998, 8 = 0.99999976, 9 = 0.70000017, eval_loss = 0.40362385, global_step = 15, loss = 0.40362385\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 15: ./human/fs/output\\model.ckpt-15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': 0.49999988,\n",
       " '1': 0.50000036,\n",
       " '10': 0.3750002,\n",
       " '2': 0.20000075,\n",
       " '3': 0.99999976,\n",
       " '4': 0.9999998,\n",
       " '5': 1.9999977e-07,\n",
       " '6': 0.49999988,\n",
       " '7': 0.9999998,\n",
       " '8': 0.99999976,\n",
       " '9': 0.70000017,\n",
       " 'eval_loss': 0.40362385,\n",
       " 'loss': 0.40362385,\n",
       " 'global_step': 15}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This tells the estimator to run through the entire set.\n",
    "eval_steps = None\n",
    "\n",
    "eval_drop_remainder = False\n",
    "eval_input_fn = file_based_input_fn_builder(\n",
    "    input_file=eval_file,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Eval results *****\n",
      "INFO:tensorflow:  0 = 0.49999988\n",
      "INFO:tensorflow:  1 = 0.50000036\n",
      "INFO:tensorflow:  10 = 0.3750002\n",
      "INFO:tensorflow:  2 = 0.20000075\n",
      "INFO:tensorflow:  3 = 0.99999976\n",
      "INFO:tensorflow:  4 = 0.9999998\n",
      "INFO:tensorflow:  5 = 1.9999977e-07\n",
      "INFO:tensorflow:  6 = 0.49999988\n",
      "INFO:tensorflow:  7 = 0.9999998\n",
      "INFO:tensorflow:  8 = 0.99999976\n",
      "INFO:tensorflow:  9 = 0.70000017\n",
      "INFO:tensorflow:  eval_loss = 0.40362385\n",
      "INFO:tensorflow:  global_step = 15\n",
      "INFO:tensorflow:  loss = 0.40362385\n"
     ]
    }
   ],
   "source": [
    "output_eval_file = os.path.join(\"./human/fs\", \"eval_results.txt\")\n",
    "with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
    "    tf.logging.info(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
