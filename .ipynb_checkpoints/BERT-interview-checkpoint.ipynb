{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# %tensorflow_version 1.x\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "from keras import Sequential\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import LSTM, Dense, Masking\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Dense, Input, concatenate, Layer, Lambda, Dropout, Activation\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, TensorBoard\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unnamed:_0</th>\n",
       "      <th>interview_id</th>\n",
       "      <th>interview_date</th>\n",
       "      <th>record_creation_date</th>\n",
       "      <th>venue</th>\n",
       "      <th>address_name</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state_/_province</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>...</th>\n",
       "      <th>interviewer_relationship_5_to_storyteller_2</th>\n",
       "      <th>language_1</th>\n",
       "      <th>language_2</th>\n",
       "      <th>language_3</th>\n",
       "      <th>keywords_-_fixed_subjects</th>\n",
       "      <th>keywords_-_general</th>\n",
       "      <th>keywords_-_places</th>\n",
       "      <th>deep_speech</th>\n",
       "      <th>human_transcript</th>\n",
       "      <th>google_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>MBY005779</td>\n",
       "      <td>10/2/09 11:30</td>\n",
       "      <td>10/2/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Penrose Public Library</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>80911.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Achievements and Awards\\n\\nArchitecture\\n\\nChi...</td>\n",
       "      <td>1941 Chevy\\n\\n1960 Starline\\n\\nagriculture\\n\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i my name is lisande i am sixty eight years ol...</td>\n",
       "      <td>LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...</td>\n",
       "      <td>my name is Teresa Sanchez I am 68 years old to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>MBY005843</td>\n",
       "      <td>10/14/09 11:30</td>\n",
       "      <td>10/14/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Penrose Public Library</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>80911.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Achievements and Awards\\n\\nChildren\\n\\nChristm...</td>\n",
       "      <td>Air Corps\\n\\ncarols\\n\\nchurch\\n\\nColorado Spri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my name is simply a le and shut stenia i go by...</td>\n",
       "      <td>Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...</td>\n",
       "      <td>my name is Cynthia Lee and shoots tehnika I go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>MBY005844</td>\n",
       "      <td>10/14/09 12:30</td>\n",
       "      <td>10/14/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Penrose Public Library</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>80911.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Army\\n\\nBest Friends\\n\\nBirth\\n\\nChanges In Ed...</td>\n",
       "      <td>Angela Gonzales\\n\\nappearance\\n\\nbirth of firs...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>okay i'm tones rein twenty nine in today's oct...</td>\n",
       "      <td>Denise Ricks: [00:00:00] Okay. I'm Denise Rick...</td>\n",
       "      <td>okay I'm Denise Rick's I'm 29 today is October...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>MBY005884</td>\n",
       "      <td>10/25/09 12:30</td>\n",
       "      <td>10/25/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>204 South Main Street</td>\n",
       "      <td>Wichita</td>\n",
       "      <td>KS</td>\n",
       "      <td>67202.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Achievements and Awards\\n\\nBirth\\n\\nChanges In...</td>\n",
       "      <td>AFLCIO\\n\\nBarack Obama\\n\\nBlind Rights\\n\\nbrai...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i mane is highly christine johnson i'm thirty ...</td>\n",
       "      <td>Heidi Christine Johnson: [00:00:00] Hi. My nam...</td>\n",
       "      <td>hi my name is Heidi Christine Johnson I'm 38 y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>MBY005906</td>\n",
       "      <td>10/31/09 9:30</td>\n",
       "      <td>10/31/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>204 South Main Street</td>\n",
       "      <td>Wichita</td>\n",
       "      <td>KS</td>\n",
       "      <td>67202.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Birth\\n\\nBosses\\n\\nComing Of Age\\n\\nCommunity ...</td>\n",
       "      <td>cohorts (groups of friends)\\n\\ncollege\\n\\ncraf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i am the morning my name's carcava i am twenty...</td>\n",
       "      <td>Sarah Culver: [00:00:00] Hi. Good morning. My ...</td>\n",
       "      <td>hi good morning my name is Sarah Culver I'm 24...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    unnamed:_0 interview_id  interview_date record_creation_date  \\\n",
       "9            9    MBY005779   10/2/09 11:30              10/2/09   \n",
       "16          16    MBY005843  10/14/09 11:30             10/14/09   \n",
       "17          17    MBY005844  10/14/09 12:30             10/14/09   \n",
       "22          22    MBY005884  10/25/09 12:30             10/25/09   \n",
       "23          23    MBY005906   10/31/09 9:30             10/31/09   \n",
       "\n",
       "                     venue      address_name                  street  \\\n",
       "9   MobileBooth West (MBY)  MobileBooth West  Penrose Public Library   \n",
       "16  MobileBooth West (MBY)  MobileBooth West  Penrose Public Library   \n",
       "17  MobileBooth West (MBY)  MobileBooth West  Penrose Public Library   \n",
       "22  MobileBooth West (MBY)  MobileBooth West   204 South Main Street   \n",
       "23  MobileBooth West (MBY)  MobileBooth West   204 South Main Street   \n",
       "\n",
       "                city state_/_province  postal_code  ...  \\\n",
       "9   Colorado Springs               CO      80911.0  ...   \n",
       "16  Colorado Springs               CO      80911.0  ...   \n",
       "17  Colorado Springs               CO      80911.0  ...   \n",
       "22           Wichita               KS      67202.0  ...   \n",
       "23           Wichita               KS      67202.0  ...   \n",
       "\n",
       "   interviewer_relationship_5_to_storyteller_2 language_1 language_2  \\\n",
       "9                                          NaN    English        NaN   \n",
       "16                                         NaN    English        NaN   \n",
       "17                                         NaN    English        NaN   \n",
       "22                                         NaN    English        NaN   \n",
       "23                                         NaN    English        NaN   \n",
       "\n",
       "   language_3                          keywords_-_fixed_subjects  \\\n",
       "9         NaN  Achievements and Awards\\n\\nArchitecture\\n\\nChi...   \n",
       "16        NaN  Achievements and Awards\\n\\nChildren\\n\\nChristm...   \n",
       "17        NaN  Army\\n\\nBest Friends\\n\\nBirth\\n\\nChanges In Ed...   \n",
       "22        NaN  Achievements and Awards\\n\\nBirth\\n\\nChanges In...   \n",
       "23        NaN  Birth\\n\\nBosses\\n\\nComing Of Age\\n\\nCommunity ...   \n",
       "\n",
       "                                   keywords_-_general keywords_-_places  \\\n",
       "9   1941 Chevy\\n\\n1960 Starline\\n\\nagriculture\\n\\n...               NaN   \n",
       "16  Air Corps\\n\\ncarols\\n\\nchurch\\n\\nColorado Spri...               NaN   \n",
       "17  Angela Gonzales\\n\\nappearance\\n\\nbirth of firs...               NaN   \n",
       "22  AFLCIO\\n\\nBarack Obama\\n\\nBlind Rights\\n\\nbrai...               NaN   \n",
       "23  cohorts (groups of friends)\\n\\ncollege\\n\\ncraf...               NaN   \n",
       "\n",
       "                                          deep_speech  \\\n",
       "9   i my name is lisande i am sixty eight years ol...   \n",
       "16  my name is simply a le and shut stenia i go by...   \n",
       "17  okay i'm tones rein twenty nine in today's oct...   \n",
       "22  i mane is highly christine johnson i'm thirty ...   \n",
       "23  i am the morning my name's carcava i am twenty...   \n",
       "\n",
       "                                     human_transcript  \\\n",
       "9   LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...   \n",
       "16  Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...   \n",
       "17  Denise Ricks: [00:00:00] Okay. I'm Denise Rick...   \n",
       "22  Heidi Christine Johnson: [00:00:00] Hi. My nam...   \n",
       "23  Sarah Culver: [00:00:00] Hi. Good morning. My ...   \n",
       "\n",
       "                                    google_transcript  \n",
       "9   my name is Teresa Sanchez I am 68 years old to...  \n",
       "16  my name is Cynthia Lee and shoots tehnika I go...  \n",
       "17  okay I'm Denise Rick's I'm 29 today is October...  \n",
       "22  hi my name is Heidi Christine Johnson I'm 38 y...  \n",
       "23  hi good morning my name is Sarah Culver I'm 24...  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_raw = pd.read_csv('Data/transcript.csv')\n",
    "transcript_raw.columns = transcript_raw.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "transcript_raw = transcript_raw[transcript_raw.human_transcript.notnull()]\n",
    "transcript_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 61)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interview_id</th>\n",
       "      <th>interview_date</th>\n",
       "      <th>record_creation_date</th>\n",
       "      <th>venue</th>\n",
       "      <th>address_name</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state_/_province</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>interview_description</th>\n",
       "      <th>...</th>\n",
       "      <th>interviewer_relationship_1_to_storyteller_1</th>\n",
       "      <th>language_1</th>\n",
       "      <th>keywords_-_fixed_subjects</th>\n",
       "      <th>keywords_-_general</th>\n",
       "      <th>label1-fs</th>\n",
       "      <th>label2-fs</th>\n",
       "      <th>label3-fs</th>\n",
       "      <th>label1-g</th>\n",
       "      <th>label2-g</th>\n",
       "      <th>label3-g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MBY005668</td>\n",
       "      <td>2009/9/5 11:30</td>\n",
       "      <td>2009/9/5</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Paonia Public Library</td>\n",
       "      <td>Paonia</td>\n",
       "      <td>CO</td>\n",
       "      <td>81428</td>\n",
       "      <td>Rita Elvia Loya (38) habla con su esposo Omar ...</td>\n",
       "      <td>...</td>\n",
       "      <td>wife</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Achievements and Awards\\n\\nChildren\\n\\nCommuni...</td>\n",
       "      <td>bilingual\\n\\nBilingue\\n\\nChihuahua, Mexico\\n\\n...</td>\n",
       "      <td>Children</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>Immigration Stories</td>\n",
       "      <td>social beliefs and practices</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>memories of former times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MBY005712</td>\n",
       "      <td>2009/9/13 17:30</td>\n",
       "      <td>2009/9/13</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Paonia Public Library</td>\n",
       "      <td>Paonia</td>\n",
       "      <td>CO</td>\n",
       "      <td>81428</td>\n",
       "      <td>Cecilio Montes (46) habla con esposa Bertha Ja...</td>\n",
       "      <td>...</td>\n",
       "      <td>esposa</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Anniversaries\\n\\nChildren\\n\\nCommunity Busines...</td>\n",
       "      <td>accidents\\n\\nbaile\\n\\ncar accident\\n\\ncemetery...</td>\n",
       "      <td>Children</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>Immigration Stories</td>\n",
       "      <td>memories of former times</td>\n",
       "      <td>spouse</td>\n",
       "      <td>family tradition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MBY005713</td>\n",
       "      <td>2009/9/14 9:30</td>\n",
       "      <td>2009/9/14</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Paonia Public Library</td>\n",
       "      <td>Paonia</td>\n",
       "      <td>CO</td>\n",
       "      <td>81428</td>\n",
       "      <td>Alice talks about childhood.  Melissa remember...</td>\n",
       "      <td>...</td>\n",
       "      <td>grandmother</td>\n",
       "      <td>English</td>\n",
       "      <td>Birth\\n\\nBurials\\n\\nCancer\\n\\nChildcare\\n\\nChi...</td>\n",
       "      <td>alcohol\\n\\nbirth of first child\\n\\nchemotherap...</td>\n",
       "      <td>Parents</td>\n",
       "      <td>Children</td>\n",
       "      <td>Marriage</td>\n",
       "      <td>memories of growing up</td>\n",
       "      <td>memories of former times</td>\n",
       "      <td>family tradition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MBY005730</td>\n",
       "      <td>2009/9/18 12:30</td>\n",
       "      <td>2009/9/18</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Paonia Public Library</td>\n",
       "      <td>Paonia</td>\n",
       "      <td>CO</td>\n",
       "      <td>81428</td>\n",
       "      <td>Adolph Carranza Jr. (64) talks with friend Ele...</td>\n",
       "      <td>...</td>\n",
       "      <td>friend</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Changes In Education\\n\\nCommunity Businesses\\n...</td>\n",
       "      <td>anecdotes (humorous but true stories)\\n\\nappea...</td>\n",
       "      <td>Parents</td>\n",
       "      <td>Schools</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>memories of growing up</td>\n",
       "      <td>social beliefs and practices</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MBY005764</td>\n",
       "      <td>2009/9/28 10:30</td>\n",
       "      <td>2009/9/28</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Public Library</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>80911</td>\n",
       "      <td>Rosana interviews her friend Alberta about her...</td>\n",
       "      <td>...</td>\n",
       "      <td>friend</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Education\\n\\nAchievements and Awards\\n\\nArmy\\n...</td>\n",
       "      <td>advocacy\\n\\nadvocate\\n\\ncollege\\n\\ndevelopment...</td>\n",
       "      <td>Schools</td>\n",
       "      <td>Children</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>memories of former times</td>\n",
       "      <td>school day memories</td>\n",
       "      <td>spouse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  interview_id   interview_date record_creation_date                   venue  \\\n",
       "0    MBY005668   2009/9/5 11:30             2009/9/5  MobileBooth West (MBY)   \n",
       "1    MBY005712  2009/9/13 17:30            2009/9/13  MobileBooth West (MBY)   \n",
       "2    MBY005713   2009/9/14 9:30            2009/9/14  MobileBooth West (MBY)   \n",
       "3    MBY005730  2009/9/18 12:30            2009/9/18  MobileBooth West (MBY)   \n",
       "4    MBY005764  2009/9/28 10:30            2009/9/28  MobileBooth West (MBY)   \n",
       "\n",
       "       address_name                 street              city state_/_province  \\\n",
       "0  MobileBooth West  Paonia Public Library            Paonia               CO   \n",
       "1  MobileBooth West  Paonia Public Library            Paonia               CO   \n",
       "2  MobileBooth West  Paonia Public Library            Paonia               CO   \n",
       "3  MobileBooth West  Paonia Public Library            Paonia               CO   \n",
       "4  MobileBooth West         Public Library  Colorado Springs               CO   \n",
       "\n",
       "   postal_code                              interview_description  ...  \\\n",
       "0        81428  Rita Elvia Loya (38) habla con su esposo Omar ...  ...   \n",
       "1        81428  Cecilio Montes (46) habla con esposa Bertha Ja...  ...   \n",
       "2        81428  Alice talks about childhood.  Melissa remember...  ...   \n",
       "3        81428  Adolph Carranza Jr. (64) talks with friend Ele...  ...   \n",
       "4        80911  Rosana interviews her friend Alberta about her...  ...   \n",
       "\n",
       "  interviewer_relationship_1_to_storyteller_1 language_1  \\\n",
       "0                                        wife    Spanish   \n",
       "1                                      esposa    Spanish   \n",
       "2                                 grandmother    English   \n",
       "3                                      friend        NaN   \n",
       "4                                      friend        NaN   \n",
       "\n",
       "                           keywords_-_fixed_subjects  \\\n",
       "0  Achievements and Awards\\n\\nChildren\\n\\nCommuni...   \n",
       "1  Anniversaries\\n\\nChildren\\n\\nCommunity Busines...   \n",
       "2  Birth\\n\\nBurials\\n\\nCancer\\n\\nChildcare\\n\\nChi...   \n",
       "3  Changes In Education\\n\\nCommunity Businesses\\n...   \n",
       "4  Education\\n\\nAchievements and Awards\\n\\nArmy\\n...   \n",
       "\n",
       "                                  keywords_-_general label1-fs     label2-fs  \\\n",
       "0  bilingual\\n\\nBilingue\\n\\nChihuahua, Mexico\\n\\n...  Children  Workday Life   \n",
       "1  accidents\\n\\nbaile\\n\\ncar accident\\n\\ncemetery...  Children  Workday Life   \n",
       "2  alcohol\\n\\nbirth of first child\\n\\nchemotherap...   Parents      Children   \n",
       "3  anecdotes (humorous but true stories)\\n\\nappea...   Parents       Schools   \n",
       "4  advocacy\\n\\nadvocate\\n\\ncollege\\n\\ndevelopment...   Schools      Children   \n",
       "\n",
       "             label3-fs                      label1-g  \\\n",
       "0  Immigration Stories  social beliefs and practices   \n",
       "1  Immigration Stories      memories of former times   \n",
       "2             Marriage        memories of growing up   \n",
       "3         Workday Life        memories of growing up   \n",
       "4         Workday Life      memories of former times   \n",
       "\n",
       "                       label2-g                  label3-g  \n",
       "0                     ethnicity  memories of former times  \n",
       "1                        spouse          family tradition  \n",
       "2      memories of former times          family tradition  \n",
       "3  social beliefs and practices                 ethnicity  \n",
       "4           school day memories                    spouse  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_raw = pd.read_csv('Data/labels.csv')\n",
    "label_raw.columns = label_raw.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "label_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unnamed:_0</th>\n",
       "      <th>interview_id</th>\n",
       "      <th>interview_date_x</th>\n",
       "      <th>record_creation_date_x</th>\n",
       "      <th>venue_x</th>\n",
       "      <th>address_name_x</th>\n",
       "      <th>street_x</th>\n",
       "      <th>city_x</th>\n",
       "      <th>state_/_province_x</th>\n",
       "      <th>postal_code_x</th>\n",
       "      <th>...</th>\n",
       "      <th>interviewer_relationship_1_to_storyteller_1_y</th>\n",
       "      <th>language_1_y</th>\n",
       "      <th>keywords_-_fixed_subjects_y</th>\n",
       "      <th>keywords_-_general_y</th>\n",
       "      <th>label1-fs</th>\n",
       "      <th>label2-fs</th>\n",
       "      <th>label3-fs</th>\n",
       "      <th>label1-g</th>\n",
       "      <th>label2-g</th>\n",
       "      <th>label3-g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>MBY005779</td>\n",
       "      <td>10/2/09 11:30</td>\n",
       "      <td>10/2/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Penrose Public Library</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>80911.0</td>\n",
       "      <td>...</td>\n",
       "      <td>husband</td>\n",
       "      <td>English</td>\n",
       "      <td>Achievements and Awards\\n\\nArchitecture\\n\\nChi...</td>\n",
       "      <td>1941 Chevy\\n\\n1960 Starline\\n\\nagriculture\\n\\n...</td>\n",
       "      <td>Parents</td>\n",
       "      <td>Schools</td>\n",
       "      <td>Children</td>\n",
       "      <td>memories of growing up</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>anecdotes (humorous but true stories)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>MBY005843</td>\n",
       "      <td>10/14/09 11:30</td>\n",
       "      <td>10/14/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Penrose Public Library</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>80911.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mother</td>\n",
       "      <td>English</td>\n",
       "      <td>Achievements and Awards\\n\\nChildren\\n\\nChristm...</td>\n",
       "      <td>Air Corps\\n\\ncarols\\n\\nchurch\\n\\nColorado Spri...</td>\n",
       "      <td>Parents</td>\n",
       "      <td>Children</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>memories of growing up</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>memories of former times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>MBY005844</td>\n",
       "      <td>10/14/09 12:30</td>\n",
       "      <td>10/14/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>Penrose Public Library</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>80911.0</td>\n",
       "      <td>...</td>\n",
       "      <td>father</td>\n",
       "      <td>English</td>\n",
       "      <td>Army\\n\\nBest Friends\\n\\nBirth\\n\\nChanges In Ed...</td>\n",
       "      <td>Angela Gonzales\\n\\nappearance\\n\\nbirth of firs...</td>\n",
       "      <td>Parents</td>\n",
       "      <td>Children</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>memories of growing up</td>\n",
       "      <td>social beliefs and practices</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>MBY005884</td>\n",
       "      <td>10/25/09 12:30</td>\n",
       "      <td>10/25/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>204 South Main Street</td>\n",
       "      <td>Wichita</td>\n",
       "      <td>KS</td>\n",
       "      <td>67202.0</td>\n",
       "      <td>...</td>\n",
       "      <td>friend</td>\n",
       "      <td>English</td>\n",
       "      <td>Achievements and Awards\\n\\nBirth\\n\\nChanges In...</td>\n",
       "      <td>AFLCIO\\n\\nBarack Obama\\n\\nBlind Rights\\n\\nbrai...</td>\n",
       "      <td>Schools</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>Immigration Stories</td>\n",
       "      <td>memories of growing up</td>\n",
       "      <td>social beliefs and practices</td>\n",
       "      <td>memories of former times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>MBY005906</td>\n",
       "      <td>10/31/09 9:30</td>\n",
       "      <td>10/31/09</td>\n",
       "      <td>MobileBooth West (MBY)</td>\n",
       "      <td>MobileBooth West</td>\n",
       "      <td>204 South Main Street</td>\n",
       "      <td>Wichita</td>\n",
       "      <td>KS</td>\n",
       "      <td>67202.0</td>\n",
       "      <td>...</td>\n",
       "      <td>interviewee</td>\n",
       "      <td>English</td>\n",
       "      <td>Birth\\n\\nBosses\\n\\nComing Of Age\\n\\nCommunity ...</td>\n",
       "      <td>cohorts (groups of friends)\\n\\ncollege\\n\\ncraf...</td>\n",
       "      <td>Parents</td>\n",
       "      <td>Schools</td>\n",
       "      <td>Workday Life</td>\n",
       "      <td>memories of growing up</td>\n",
       "      <td>social beliefs and practices</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unnamed:_0 interview_id interview_date_x record_creation_date_x  \\\n",
       "0           9    MBY005779    10/2/09 11:30                10/2/09   \n",
       "1          16    MBY005843   10/14/09 11:30               10/14/09   \n",
       "2          17    MBY005844   10/14/09 12:30               10/14/09   \n",
       "3          22    MBY005884   10/25/09 12:30               10/25/09   \n",
       "4          23    MBY005906    10/31/09 9:30               10/31/09   \n",
       "\n",
       "                  venue_x    address_name_x                street_x  \\\n",
       "0  MobileBooth West (MBY)  MobileBooth West  Penrose Public Library   \n",
       "1  MobileBooth West (MBY)  MobileBooth West  Penrose Public Library   \n",
       "2  MobileBooth West (MBY)  MobileBooth West  Penrose Public Library   \n",
       "3  MobileBooth West (MBY)  MobileBooth West   204 South Main Street   \n",
       "4  MobileBooth West (MBY)  MobileBooth West   204 South Main Street   \n",
       "\n",
       "             city_x state_/_province_x  postal_code_x  ...  \\\n",
       "0  Colorado Springs                 CO        80911.0  ...   \n",
       "1  Colorado Springs                 CO        80911.0  ...   \n",
       "2  Colorado Springs                 CO        80911.0  ...   \n",
       "3           Wichita                 KS        67202.0  ...   \n",
       "4           Wichita                 KS        67202.0  ...   \n",
       "\n",
       "  interviewer_relationship_1_to_storyteller_1_y language_1_y  \\\n",
       "0                                       husband      English   \n",
       "1                                        mother      English   \n",
       "2                                        father      English   \n",
       "3                                        friend      English   \n",
       "4                                   interviewee      English   \n",
       "\n",
       "                         keywords_-_fixed_subjects_y  \\\n",
       "0  Achievements and Awards\\n\\nArchitecture\\n\\nChi...   \n",
       "1  Achievements and Awards\\n\\nChildren\\n\\nChristm...   \n",
       "2  Army\\n\\nBest Friends\\n\\nBirth\\n\\nChanges In Ed...   \n",
       "3  Achievements and Awards\\n\\nBirth\\n\\nChanges In...   \n",
       "4  Birth\\n\\nBosses\\n\\nComing Of Age\\n\\nCommunity ...   \n",
       "\n",
       "                                keywords_-_general_y label1-fs     label2-fs  \\\n",
       "0  1941 Chevy\\n\\n1960 Starline\\n\\nagriculture\\n\\n...   Parents       Schools   \n",
       "1  Air Corps\\n\\ncarols\\n\\nchurch\\n\\nColorado Spri...   Parents      Children   \n",
       "2  Angela Gonzales\\n\\nappearance\\n\\nbirth of firs...   Parents      Children   \n",
       "3  AFLCIO\\n\\nBarack Obama\\n\\nBlind Rights\\n\\nbrai...   Schools  Workday Life   \n",
       "4  cohorts (groups of friends)\\n\\ncollege\\n\\ncraf...   Parents       Schools   \n",
       "\n",
       "             label3-fs                label1-g                      label2-g  \\\n",
       "0             Children  memories of growing up                     ethnicity   \n",
       "1         Workday Life  memories of growing up                     ethnicity   \n",
       "2         Workday Life  memories of growing up  social beliefs and practices   \n",
       "3  Immigration Stories  memories of growing up  social beliefs and practices   \n",
       "4         Workday Life  memories of growing up  social beliefs and practices   \n",
       "\n",
       "                                label3-g  \n",
       "0  anecdotes (humorous but true stories)  \n",
       "1               memories of former times  \n",
       "2                              ethnicity  \n",
       "3               memories of former times  \n",
       "4                              ethnicity  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw = transcript_raw.merge(label_raw, on='interview_id')\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 84)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_transcript</th>\n",
       "      <th>label1-g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...</td>\n",
       "      <td>memories of growing up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...</td>\n",
       "      <td>memories of growing up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Denise Ricks: [00:00:00] Okay. I'm Denise Rick...</td>\n",
       "      <td>memories of growing up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heidi Christine Johnson: [00:00:00] Hi. My nam...</td>\n",
       "      <td>memories of growing up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sarah Culver: [00:00:00] Hi. Good morning. My ...</td>\n",
       "      <td>memories of growing up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    human_transcript                label1-g\n",
       "0  LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...  memories of growing up\n",
       "1  Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...  memories of growing up\n",
       "2  Denise Ricks: [00:00:00] Okay. I'm Denise Rick...  memories of growing up\n",
       "3  Heidi Christine Johnson: [00:00:00] Hi. My nam...  memories of growing up\n",
       "4  Sarah Culver: [00:00:00] Hi. Good morning. My ...  memories of growing up"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw = train_raw[['human_transcript', 'label1-g']]\n",
    "train_raw.reset_index(inplace=True, drop=True)\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethnicity\n",
      "memories of growing up\n",
      "school day memories\n",
      "social beliefs and practices\n"
     ]
    }
   ],
   "source": [
    "for l in np.unique(train_raw['label1-g']):\n",
    "  print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...</td>\n",
       "      <td>memories of growing up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...</td>\n",
       "      <td>memories of growing up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Denise Ricks: [00:00:00] Okay. I'm Denise Rick...</td>\n",
       "      <td>memories of growing up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heidi Christine Johnson: [00:00:00] Hi. My nam...</td>\n",
       "      <td>memories of growing up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sarah Culver: [00:00:00] Hi. Good morning. My ...</td>\n",
       "      <td>memories of growing up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                   label\n",
       "0  LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...  memories of growing up\n",
       "1  Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...  memories of growing up\n",
       "2  Denise Ricks: [00:00:00] Okay. I'm Denise Rick...  memories of growing up\n",
       "3  Heidi Christine Johnson: [00:00:00] Hi. My nam...  memories of growing up\n",
       "4  Sarah Culver: [00:00:00] Hi. Good morning. My ...  memories of growing up"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw=train_raw.rename(columns = {'human_transcript':'text', 'label1-g':'label'})\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Denise Ricks: [00:00:00] Okay. I'm Denise Rick...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heidi Christine Johnson: [00:00:00] Hi. My nam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sarah Culver: [00:00:00] Hi. Good morning. My ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...      1\n",
       "1  Cynthia Lee Anschutz Stenicka:\\t[00:00:00] My ...      1\n",
       "2  Denise Ricks: [00:00:00] Okay. I'm Denise Rick...      1\n",
       "3  Heidi Christine Johnson: [00:00:00] Hi. My nam...      1\n",
       "4  Sarah Culver: [00:00:00] Hi. Good morning. My ...      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LE = LabelEncoder()\n",
    "train_raw['label'] = LE.fit_transform(train_raw['label'])\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(train_raw['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Tomas:\\t[00:00:00] My name is Tomas Garduno. I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Natalie Romero:\\t[00:00:00] Hi. My name is Nat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Interviewer:\\t[00:00:00] Go.Jose Armas:\\tMy na...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bolivar Moyano Fraga: [00:00:00] My name is Bo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "35  Tomas:\\t[00:00:00] My name is Tomas Garduno. I...      1\n",
       "30  Natalie Romero:\\t[00:00:00] Hi. My name is Nat...      1\n",
       "0   LOUISA SANCHEZ: [00:00:00] My name is Luisa Sa...      1\n",
       "37  Interviewer:\\t[00:00:00] Go.Jose Armas:\\tMy na...      3\n",
       "10  Bolivar Moyano Fraga: [00:00:00] My name is Bo...      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.reindex(np.random.permutation(train.index))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_txt(text):\n",
    "  text = re.sub(\"'\", \"\",text)\n",
    "  text=re.sub(\"(\\\\W)+\",\" \",text)    \n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Tomas 00 00 00 My name is Tomas Garduno Im 31 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Natalie Romero 00 00 00 Hi My name is Natalie ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOUISA SANCHEZ 00 00 00 My name is Luisa Sanch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Interviewer 00 00 00 Go Jose Armas My name is ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bolivar Moyano Fraga 00 00 00 My name is Boliv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "35  Tomas 00 00 00 My name is Tomas Garduno Im 31 ...      1\n",
       "30  Natalie Romero 00 00 00 Hi My name is Natalie ...      1\n",
       "0   LOUISA SANCHEZ 00 00 00 My name is Luisa Sanch...      1\n",
       "37  Interviewer 00 00 00 Go Jose Armas My name is ...      3\n",
       "10  Bolivar Moyano Fraga 00 00 00 My name is Boliv...      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text']  = train.text.apply(clean_txt)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>John Rivera Sedlar 00 00 00 Hi my name is John...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Santos Martinez 00 00 00 Im Santos Martinez Im...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Eloise Serna 00 00 00 My name is Eloise Serna ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Naomi Guerrero 00 00 00 Hello My name is Naomi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Sabina Maria Zuniga Varela 00 00 00 My name is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "14  John Rivera Sedlar 00 00 00 Hi my name is John...      3\n",
       "47  Santos Martinez 00 00 00 Im Santos Martinez Im...      3\n",
       "39  Eloise Serna 00 00 00 My name is Eloise Serna ...      1\n",
       "26  Naomi Guerrero 00 00 00 Hello My name is Naomi...      1\n",
       "46  Sabina Maria Zuniga Varela 00 00 00 My name is...      1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=35)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Rivera Sedlar 00 00 00 Hi my name is John...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Santos Martinez 00 00 00 Im Santos Martinez Im...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  John Rivera Sedlar 00 00 00 Hi my name is John...      3\n",
       "1  Santos Martinez 00 00 00 Im Santos Martinez Im...      3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.reset_index(drop=True, inplace=True)\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BARBARA ASHUD 00 00 00 My name is Barbra Ashud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FEMALE_1 00 00 00 Good afternoon My name is Ni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  BARBARA ASHUD 00 00 00 My name is Barbra Ashud...      1\n",
       "1  FEMALE_1 00 00 00 Good afternoon My name is Ni...      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.reset_index(drop=True, inplace=True)\n",
    "val.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12, 2), (45, 2))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in c:\\users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages (from bert-tensorflow) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "#Installing BERT module\n",
    "!pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\bert\\optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Importing BERT modules\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: ./bert_output/ *****\n"
     ]
    }
   ],
   "source": [
    "# Set the output directory for saving model file\n",
    "OUTPUT_DIR = './bert_output/'\n",
    "\n",
    "#@markdown Whether or not to clear/delete the directory and create a new one\n",
    "DO_DELETE = True #@param {type:\"boolean\"}\n",
    "\n",
    "if DO_DELETE:\n",
    "  try:\n",
    "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape : (45, 2)\n",
      "Validation Set Shape : (12, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set Shape :\", train.shape)\n",
    "print(\"Validation Set Shape :\", val.shape)\n",
    "# print(\"Test Set Shape :\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_COLUMN = 'text'\n",
    "LABEL_COLUMN = 'label'\n",
    "# The list containing all the classes (train['SECTION'].unique())\n",
    "label_list = [x for x in np.unique(train.label)]\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(text1):\n",
    "  l_total = []\n",
    "  l_parcial = []\n",
    "  if len(text1.split())//150 >0:\n",
    "    n = len(text1.split())//150\n",
    "  else: \n",
    "    n = 1\n",
    "  for w in range(n):\n",
    "    if w == 0:\n",
    "      l_parcial = text1.split()[:200]\n",
    "      l_total.append(\" \".join(l_parcial))\n",
    "    else:\n",
    "      l_parcial = text1.split()[w*150:w*150 + 200]\n",
    "      l_total.append(\" \".join(l_parcial))\n",
    "  return l_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Rivera Sedlar 00 00 00 Hi my name is John...</td>\n",
       "      <td>3</td>\n",
       "      <td>[John Rivera Sedlar 00 00 00 Hi my name is Joh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Santos Martinez 00 00 00 Im Santos Martinez Im...</td>\n",
       "      <td>3</td>\n",
       "      <td>[Santos Martinez 00 00 00 Im Santos Martinez I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eloise Serna 00 00 00 My name is Eloise Serna ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Eloise Serna 00 00 00 My name is Eloise Serna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naomi Guerrero 00 00 00 Hello My name is Naomi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Naomi Guerrero 00 00 00 Hello My name is Naom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sabina Maria Zuniga Varela 00 00 00 My name is...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Sabina Maria Zuniga Varela 00 00 00 My name i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  John Rivera Sedlar 00 00 00 Hi my name is John...      3   \n",
       "1  Santos Martinez 00 00 00 Im Santos Martinez Im...      3   \n",
       "2  Eloise Serna 00 00 00 My name is Eloise Serna ...      1   \n",
       "3  Naomi Guerrero 00 00 00 Hello My name is Naomi...      1   \n",
       "4  Sabina Maria Zuniga Varela 00 00 00 My name is...      1   \n",
       "\n",
       "                                          text_split  \n",
       "0  [John Rivera Sedlar 00 00 00 Hi my name is Joh...  \n",
       "1  [Santos Martinez 00 00 00 Im Santos Martinez I...  \n",
       "2  [Eloise Serna 00 00 00 My name is Eloise Serna...  \n",
       "3  [Naomi Guerrero 00 00 00 Hello My name is Naom...  \n",
       "4  [Sabina Maria Zuniga Varela 00 00 00 My name i...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text_split'] = train[DATA_COLUMN].apply(get_split)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BARBARA ASHUD 00 00 00 My name is Barbra Ashud...</td>\n",
       "      <td>1</td>\n",
       "      <td>[BARBARA ASHUD 00 00 00 My name is Barbra Ashu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FEMALE_1 00 00 00 Good afternoon My name is Ni...</td>\n",
       "      <td>1</td>\n",
       "      <td>[FEMALE_1 00 00 00 Good afternoon My name is N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  BARBARA ASHUD 00 00 00 My name is Barbra Ashud...      1   \n",
       "1  FEMALE_1 00 00 00 Good afternoon My name is Ni...      1   \n",
       "\n",
       "                                          text_split  \n",
       "0  [BARBARA ASHUD 00 00 00 My name is Barbra Ashu...  \n",
       "1  [FEMALE_1 00 00 00 Good afternoon My name is N...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val['text_split'] = val[DATA_COLUMN].apply(get_split)\n",
    "val.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1715, 1715, 1715)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_l = []\n",
    "label_l = []\n",
    "index_l =[]\n",
    "for idx,row in train.iterrows():\n",
    "  for l in row['text_split']:\n",
    "    train_l.append(l)\n",
    "    label_l.append(row['label'])\n",
    "    index_l.append(idx)\n",
    "len(train_l), len(label_l), len(index_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(466, 466, 466)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_l = []\n",
    "val_label_l = []\n",
    "val_index_l = []\n",
    "for idx,row in val.iterrows():\n",
    "  for l in row['text_split']:\n",
    "    val_l.append(l)\n",
    "    val_label_l.append(row['label'])\n",
    "    val_index_l.append(idx)\n",
    "len(val_l), len(val_label_l), len(val_index_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Rivera Sedlar 00 00 00 Hi my name is John...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>went over to an office I think its called a Fu...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Rivera Sedlar Do you remember what year t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Latin food Lourdes Campos No John Rivera Sedla...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chili both red and green with your turkey Youd...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  John Rivera Sedlar 00 00 00 Hi my name is John...      3\n",
       "1  went over to an office I think its called a Fu...      3\n",
       "2  John Rivera Sedlar Do you remember what year t...      3\n",
       "3  Latin food Lourdes Campos No John Rivera Sedla...      3\n",
       "4  chili both red and green with your turkey Youd...      3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame({DATA_COLUMN:train_l, LABEL_COLUMN:label_l})\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BARBARA ASHUD 00 00 00 My name is Barbra Ashud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>December 28 1981 My mom always says its the da...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quite a bit Sometimes when I see little kids a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us to be happy and safe and the US represented...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>also an experience in its own FEMALE_1 How old...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  BARBARA ASHUD 00 00 00 My name is Barbra Ashud...      1\n",
       "1  December 28 1981 My mom always says its the da...      1\n",
       "2  quite a bit Sometimes when I see little kids a...      1\n",
       "3  us to be happy and safe and the US represented...      1\n",
       "4  also an experience in its own FEMALE_1 How old...      1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.DataFrame({DATA_COLUMN:val_l, LABEL_COLUMN:val_label_l})\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_InputExamples = train_df.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "val_InputExamples = val_df.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       <bert.run_classifier.InputExample object at 0x...\n",
       "1       <bert.run_classifier.InputExample object at 0x...\n",
       "2       <bert.run_classifier.InputExample object at 0x...\n",
       "3       <bert.run_classifier.InputExample object at 0x...\n",
       "4       <bert.run_classifier.InputExample object at 0x...\n",
       "                              ...                        \n",
       "1710    <bert.run_classifier.InputExample object at 0x...\n",
       "1711    <bert.run_classifier.InputExample object at 0x...\n",
       "1712    <bert.run_classifier.InputExample object at 0x...\n",
       "1713    <bert.run_classifier.InputExample object at 0x...\n",
       "1714    <bert.run_classifier.InputExample object at 0x...\n",
       "Length: 1715, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0 - guid of training set :  None\n",
      "\n",
      "__________\n",
      "Row 0 - text_a of training set :  John Rivera Sedlar 00 00 00 Hi my name is John Rivera Sedlar Im 55 years old Todays date is February 13th 2010 Were in East Los Angeles and Im friends with Lourdes Campos Lourdes Campos Hello My name is Lourdes Campos Im 50 and were here in East Los Angeles Todays February 13th 2010 and my relationship to John is were good friends So John how did we meet John Rivera Sedlar Thats a short question Lourdes and I met through the world of food and I have a very good friend named Joan Vogel who introduced me to Lourdes one day because I was looking for someone to help with our small business we made tamales We were in the tamale business and I was meeting Joan for a cup of coffee and she said Come with me right now youve got to meet my friend So we went over to an office I think its called a Fulfillment Lourdes Campos Yes thats right Its Fulfillment house John Rivera Sedlar I met Lourdes and we talked a little bit about the food industry I dont think we ever really worked together on the actual food A later project\n",
      "\n",
      "__________\n",
      "Row 0 - text_b of training set :  None\n",
      "\n",
      "__________\n",
      "Row 0 - label of training set :  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Row 0 - guid of training set : \", train_InputExamples.iloc[0].guid)\n",
    "print(\"\\n__________\\nRow 0 - text_a of training set : \", train_InputExamples.iloc[0].text_a)\n",
    "print(\"\\n__________\\nRow 0 - text_b of training set : \", train_InputExamples.iloc[0].text_b)\n",
    "print(\"\\n__________\\nRow 0 - label of training set : \", train_InputExamples.iloc[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john', 'rivera', 'se', '##dl', '##ar', '00', '00', '00', 'hi', 'my', 'name', 'is', 'john', 'rivera', 'se', '##dl', '##ar', 'im', '55', 'years', 'old', 'today', '##s', 'date', 'is', 'february', '13th', '2010', 'were', 'in', 'east', 'los', 'angeles', 'and', 'im', 'friends', 'with', 'lou', '##rdes', 'campos', 'lou', '##rdes', 'campos', 'hello', 'my', 'name', 'is', 'lou', '##rdes', 'campos', 'im', '50', 'and', 'were', 'here', 'in', 'east', 'los', 'angeles', 'today', '##s', 'february', '13th', '2010', 'and', 'my', 'relationship', 'to', 'john', 'is', 'were', 'good', 'friends', 'so', 'john', 'how', 'did', 'we', 'meet', 'john', 'rivera', 'se', '##dl', '##ar', 'that', '##s', 'a', 'short', 'question', 'lou', '##rdes', 'and', 'i', 'met', 'through', 'the', 'world', 'of', 'food', 'and', 'i', 'have', 'a', 'very', 'good', 'friend', 'named', 'joan', 'vogel', 'who', 'introduced', 'me', 'to', 'lou', '##rdes', 'one', 'day', 'because', 'i', 'was', 'looking', 'for', 'someone', 'to', 'help', 'with', 'our', 'small', 'business', 'we', 'made', 'tam', '##ales', 'we', 'were', 'in', 'the', 'tam', '##ale', 'business', 'and', 'i', 'was', 'meeting', 'joan', 'for', 'a', 'cup', 'of', 'coffee', 'and', 'she', 'said', 'come', 'with', 'me', 'right', 'now', 'you', '##ve', 'got', 'to', 'meet', 'my', 'friend', 'so', 'we', 'went', 'over', 'to', 'an', 'office', 'i', 'think', 'its', 'called', 'a', 'fulfillment', 'lou', '##rdes', 'campos', 'yes', 'that', '##s', 'right', 'its', 'fulfillment', 'house', 'john', 'rivera', 'se', '##dl', '##ar', 'i', 'met', 'lou', '##rdes', 'and', 'we', 'talked', 'a', 'little', 'bit', 'about', 'the', 'food', 'industry', 'i', 'don', '##t', 'think', 'we', 'ever', 'really', 'worked', 'together', 'on', 'the', 'actual', 'food', 'a', 'later', 'project']\n"
     ]
    }
   ],
   "source": [
    "#Here is what the tokenised sample of the first training set observation looks like\n",
    "print(tokenizer.tokenize(train_InputExamples.iloc[0].text_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\bert\\run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\bert\\run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 1715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 1715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] john rivera se ##dl ##ar 00 00 00 hi my name is john rivera se ##dl ##ar im 55 years old today ##s date is february 13th 2010 were in east los angeles and im friends with lou ##rdes campos lou ##rdes campos hello my name is lou ##rdes campos im 50 and were here in east los angeles today ##s february 13th 2010 and my relationship to john is were good friends so john how did we meet john rivera se ##dl ##ar that ##s a short question lou ##rdes and i met through the world of food and i have a very good friend named joan vogel who introduced me to lou ##rdes one day because i was looking for someone to help with our small business we made tam ##ales we were in the tam ##ale business and i was meeting joan for a cup of coffee and she said come with me right now you ##ve got to meet my friend so we went over to an office i think its called a fulfillment lou ##rdes campos yes that ##s right its fulfillment house john rivera se ##dl ##ar i met lou ##rdes and [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] john rivera se ##dl ##ar 00 00 00 hi my name is john rivera se ##dl ##ar im 55 years old today ##s date is february 13th 2010 were in east los angeles and im friends with lou ##rdes campos lou ##rdes campos hello my name is lou ##rdes campos im 50 and were here in east los angeles today ##s february 13th 2010 and my relationship to john is were good friends so john how did we meet john rivera se ##dl ##ar that ##s a short question lou ##rdes and i met through the world of food and i have a very good friend named joan vogel who introduced me to lou ##rdes one day because i was looking for someone to help with our small business we made tam ##ales we were in the tam ##ale business and i was meeting joan for a cup of coffee and she said come with me right now you ##ve got to meet my friend so we went over to an office i think its called a fulfillment lou ##rdes campos yes that ##s right its fulfillment house john rivera se ##dl ##ar i met lou ##rdes and [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2198 14043 7367 19422 2906 4002 4002 4002 7632 2026 2171 2003 2198 14043 7367 19422 2906 10047 4583 2086 2214 2651 2015 3058 2003 2337 6122 2230 2020 1999 2264 3050 3349 1998 10047 2814 2007 10223 26371 26925 10223 26371 26925 7592 2026 2171 2003 10223 26371 26925 10047 2753 1998 2020 2182 1999 2264 3050 3349 2651 2015 2337 6122 2230 1998 2026 3276 2000 2198 2003 2020 2204 2814 2061 2198 2129 2106 2057 3113 2198 14043 7367 19422 2906 2008 2015 1037 2460 3160 10223 26371 1998 1045 2777 2083 1996 2088 1997 2833 1998 1045 2031 1037 2200 2204 2767 2315 7437 27063 2040 3107 2033 2000 10223 26371 2028 2154 2138 1045 2001 2559 2005 2619 2000 2393 2007 2256 2235 2449 2057 2081 17214 23266 2057 2020 1999 1996 17214 9453 2449 1998 1045 2001 3116 7437 2005 1037 2452 1997 4157 1998 2016 2056 2272 2007 2033 2157 2085 2017 3726 2288 2000 3113 2026 2767 2061 2057 2253 2058 2000 2019 2436 1045 2228 2049 2170 1037 29362 10223 26371 26925 2748 2008 2015 2157 2049 29362 2160 2198 14043 7367 19422 2906 1045 2777 10223 26371 1998 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2198 14043 7367 19422 2906 4002 4002 4002 7632 2026 2171 2003 2198 14043 7367 19422 2906 10047 4583 2086 2214 2651 2015 3058 2003 2337 6122 2230 2020 1999 2264 3050 3349 1998 10047 2814 2007 10223 26371 26925 10223 26371 26925 7592 2026 2171 2003 10223 26371 26925 10047 2753 1998 2020 2182 1999 2264 3050 3349 2651 2015 2337 6122 2230 1998 2026 3276 2000 2198 2003 2020 2204 2814 2061 2198 2129 2106 2057 3113 2198 14043 7367 19422 2906 2008 2015 1037 2460 3160 10223 26371 1998 1045 2777 2083 1996 2088 1997 2833 1998 1045 2031 1037 2200 2204 2767 2315 7437 27063 2040 3107 2033 2000 10223 26371 2028 2154 2138 1045 2001 2559 2005 2619 2000 2393 2007 2256 2235 2449 2057 2081 17214 23266 2057 2020 1999 1996 17214 9453 2449 1998 1045 2001 3116 7437 2005 1037 2452 1997 4157 1998 2016 2056 2272 2007 2033 2157 2085 2017 3726 2288 2000 3113 2026 2767 2061 2057 2253 2058 2000 2019 2436 1045 2228 2049 2170 1037 29362 10223 26371 26925 2748 2008 2015 2157 2049 29362 2160 2198 14043 7367 19422 2906 1045 2777 10223 26371 1998 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 3 (id = 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 3 (id = 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] went over to an office i think its called a fulfillment lou ##rdes campos yes that ##s right its fulfillment house john rivera se ##dl ##ar i met lou ##rdes and we talked a little bit about the food industry i don ##t think we ever really worked together on the actual food a later project came up it also had to do with food but it was very interesting the projects that lou ##rdes had and the products she was getting in lou ##rdes campos we were working with lots of go ##ur ##met food companies and shipping for them and getting them manufactured at that time we were trying to help you do the same thing john rivera se ##dl ##ar was it small companies lou ##rdes campos small little retail go ##ur ##met companies john rivera se ##dl ##ar mail order lou ##rdes campos mail order shipping we were doing a lot of big stores whole foods essence and you were involved in that too john rivera se ##dl ##ar do you remember what year that was lou ##rdes campos i think it was 2000 it was about 2000 actually john rivera se ##dl ##ar [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] went over to an office i think its called a fulfillment lou ##rdes campos yes that ##s right its fulfillment house john rivera se ##dl ##ar i met lou ##rdes and we talked a little bit about the food industry i don ##t think we ever really worked together on the actual food a later project came up it also had to do with food but it was very interesting the projects that lou ##rdes had and the products she was getting in lou ##rdes campos we were working with lots of go ##ur ##met food companies and shipping for them and getting them manufactured at that time we were trying to help you do the same thing john rivera se ##dl ##ar was it small companies lou ##rdes campos small little retail go ##ur ##met companies john rivera se ##dl ##ar mail order lou ##rdes campos mail order shipping we were doing a lot of big stores whole foods essence and you were involved in that too john rivera se ##dl ##ar do you remember what year that was lou ##rdes campos i think it was 2000 it was about 2000 actually john rivera se ##dl ##ar [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2253 2058 2000 2019 2436 1045 2228 2049 2170 1037 29362 10223 26371 26925 2748 2008 2015 2157 2049 29362 2160 2198 14043 7367 19422 2906 1045 2777 10223 26371 1998 2057 5720 1037 2210 2978 2055 1996 2833 3068 1045 2123 2102 2228 2057 2412 2428 2499 2362 2006 1996 5025 2833 1037 2101 2622 2234 2039 2009 2036 2018 2000 2079 2007 2833 2021 2009 2001 2200 5875 1996 3934 2008 10223 26371 2018 1998 1996 3688 2016 2001 2893 1999 10223 26371 26925 2057 2020 2551 2007 7167 1997 2175 3126 11368 2833 3316 1998 7829 2005 2068 1998 2893 2068 7609 2012 2008 2051 2057 2020 2667 2000 2393 2017 2079 1996 2168 2518 2198 14043 7367 19422 2906 2001 2009 2235 3316 10223 26371 26925 2235 2210 7027 2175 3126 11368 3316 2198 14043 7367 19422 2906 5653 2344 10223 26371 26925 5653 2344 7829 2057 2020 2725 1037 2843 1997 2502 5324 2878 9440 11305 1998 2017 2020 2920 1999 2008 2205 2198 14043 7367 19422 2906 2079 2017 3342 2054 2095 2008 2001 10223 26371 26925 1045 2228 2009 2001 2456 2009 2001 2055 2456 2941 2198 14043 7367 19422 2906 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2253 2058 2000 2019 2436 1045 2228 2049 2170 1037 29362 10223 26371 26925 2748 2008 2015 2157 2049 29362 2160 2198 14043 7367 19422 2906 1045 2777 10223 26371 1998 2057 5720 1037 2210 2978 2055 1996 2833 3068 1045 2123 2102 2228 2057 2412 2428 2499 2362 2006 1996 5025 2833 1037 2101 2622 2234 2039 2009 2036 2018 2000 2079 2007 2833 2021 2009 2001 2200 5875 1996 3934 2008 10223 26371 2018 1998 1996 3688 2016 2001 2893 1999 10223 26371 26925 2057 2020 2551 2007 7167 1997 2175 3126 11368 2833 3316 1998 7829 2005 2068 1998 2893 2068 7609 2012 2008 2051 2057 2020 2667 2000 2393 2017 2079 1996 2168 2518 2198 14043 7367 19422 2906 2001 2009 2235 3316 10223 26371 26925 2235 2210 7027 2175 3126 11368 3316 2198 14043 7367 19422 2906 5653 2344 10223 26371 26925 5653 2344 7829 2057 2020 2725 1037 2843 1997 2502 5324 2878 9440 11305 1998 2017 2020 2920 1999 2008 2205 2198 14043 7367 19422 2906 2079 2017 3342 2054 2095 2008 2001 10223 26371 26925 1045 2228 2009 2001 2456 2009 2001 2055 2456 2941 2198 14043 7367 19422 2906 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 3 (id = 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 3 (id = 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] john rivera se ##dl ##ar do you remember what year that was lou ##rdes campos i think it was 2000 it was about 2000 actually john rivera se ##dl ##ar that was 10 years ago lou ##rdes campos right john rivera se ##dl ##ar i didn ##t realize that many years had gone by i find it very interesting that food was popular at that time but it wasn ##t white hot popular like it is today lou ##rdes campos no its just starting it was per ##cola ##ting john rivera se ##dl ##ar it was per ##cola ##ting and americans were starting to understand the importance of food they didn ##t quite know how it connected to their heritage and to them as people they didn ##t understand how no ##uri ##shing or not no ##uri ##shing that it was times have really changed since that first meeting lou ##rdes campos well the organic industry was just starting at that point john rivera se ##dl ##ar so actually at that time people didn ##t understand about latin food lou ##rdes campos no john rivera se ##dl ##ar they didn ##t understand about mexican american food they saw [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] john rivera se ##dl ##ar do you remember what year that was lou ##rdes campos i think it was 2000 it was about 2000 actually john rivera se ##dl ##ar that was 10 years ago lou ##rdes campos right john rivera se ##dl ##ar i didn ##t realize that many years had gone by i find it very interesting that food was popular at that time but it wasn ##t white hot popular like it is today lou ##rdes campos no its just starting it was per ##cola ##ting john rivera se ##dl ##ar it was per ##cola ##ting and americans were starting to understand the importance of food they didn ##t quite know how it connected to their heritage and to them as people they didn ##t understand how no ##uri ##shing or not no ##uri ##shing that it was times have really changed since that first meeting lou ##rdes campos well the organic industry was just starting at that point john rivera se ##dl ##ar so actually at that time people didn ##t understand about latin food lou ##rdes campos no john rivera se ##dl ##ar they didn ##t understand about mexican american food they saw [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2198 14043 7367 19422 2906 2079 2017 3342 2054 2095 2008 2001 10223 26371 26925 1045 2228 2009 2001 2456 2009 2001 2055 2456 2941 2198 14043 7367 19422 2906 2008 2001 2184 2086 3283 10223 26371 26925 2157 2198 14043 7367 19422 2906 1045 2134 2102 5382 2008 2116 2086 2018 2908 2011 1045 2424 2009 2200 5875 2008 2833 2001 2759 2012 2008 2051 2021 2009 2347 2102 2317 2980 2759 2066 2009 2003 2651 10223 26371 26925 2053 2049 2074 3225 2009 2001 2566 26289 3436 2198 14043 7367 19422 2906 2009 2001 2566 26289 3436 1998 4841 2020 3225 2000 3305 1996 5197 1997 2833 2027 2134 2102 3243 2113 2129 2009 4198 2000 2037 4348 1998 2000 2068 2004 2111 2027 2134 2102 3305 2129 2053 9496 12227 2030 2025 2053 9496 12227 2008 2009 2001 2335 2031 2428 2904 2144 2008 2034 3116 10223 26371 26925 2092 1996 7554 3068 2001 2074 3225 2012 2008 2391 2198 14043 7367 19422 2906 2061 2941 2012 2008 2051 2111 2134 2102 3305 2055 3763 2833 10223 26371 26925 2053 2198 14043 7367 19422 2906 2027 2134 2102 3305 2055 4916 2137 2833 2027 2387 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2198 14043 7367 19422 2906 2079 2017 3342 2054 2095 2008 2001 10223 26371 26925 1045 2228 2009 2001 2456 2009 2001 2055 2456 2941 2198 14043 7367 19422 2906 2008 2001 2184 2086 3283 10223 26371 26925 2157 2198 14043 7367 19422 2906 1045 2134 2102 5382 2008 2116 2086 2018 2908 2011 1045 2424 2009 2200 5875 2008 2833 2001 2759 2012 2008 2051 2021 2009 2347 2102 2317 2980 2759 2066 2009 2003 2651 10223 26371 26925 2053 2049 2074 3225 2009 2001 2566 26289 3436 2198 14043 7367 19422 2906 2009 2001 2566 26289 3436 1998 4841 2020 3225 2000 3305 1996 5197 1997 2833 2027 2134 2102 3243 2113 2129 2009 4198 2000 2037 4348 1998 2000 2068 2004 2111 2027 2134 2102 3305 2129 2053 9496 12227 2030 2025 2053 9496 12227 2008 2009 2001 2335 2031 2428 2904 2144 2008 2034 3116 10223 26371 26925 2092 1996 7554 3068 2001 2074 3225 2012 2008 2391 2198 14043 7367 19422 2906 2061 2941 2012 2008 2051 2111 2134 2102 3305 2055 3763 2833 10223 26371 26925 2053 2198 14043 7367 19422 2906 2027 2134 2102 3305 2055 4916 2137 2833 2027 2387 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 3 (id = 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 3 (id = 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] latin food lou ##rdes campos no john rivera se ##dl ##ar they didn ##t understand about mexican american food they saw it as a sub level second quality as backseat cuisine its changed since then now people see it that its shoulder to shoulder with the other great kitchens the other great cuisine ##s of the world lou ##rdes campos what got you interested in the fusion john rivera se ##dl ##ar well my mother is rosa rivera i grew up in santa fe new mexico but my father was in the air force so we lived in spain for three years as a very young child i lived in france but i don ##t remember too much of that he was also stationed in sacramento in utah but wed always go back to santa fe where my hispanic family were from always go back to the family fiesta ##s the holidays christmas and thanksgiving dinner where you would have chili both red and green with your turkey you ##d have chico ##s and po ##sol ##e corn ##s stew ##s with your turkey so i had this connection to being latino but because i had this background [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] latin food lou ##rdes campos no john rivera se ##dl ##ar they didn ##t understand about mexican american food they saw it as a sub level second quality as backseat cuisine its changed since then now people see it that its shoulder to shoulder with the other great kitchens the other great cuisine ##s of the world lou ##rdes campos what got you interested in the fusion john rivera se ##dl ##ar well my mother is rosa rivera i grew up in santa fe new mexico but my father was in the air force so we lived in spain for three years as a very young child i lived in france but i don ##t remember too much of that he was also stationed in sacramento in utah but wed always go back to santa fe where my hispanic family were from always go back to the family fiesta ##s the holidays christmas and thanksgiving dinner where you would have chili both red and green with your turkey you ##d have chico ##s and po ##sol ##e corn ##s stew ##s with your turkey so i had this connection to being latino but because i had this background [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3763 2833 10223 26371 26925 2053 2198 14043 7367 19422 2906 2027 2134 2102 3305 2055 4916 2137 2833 2027 2387 2009 2004 1037 4942 2504 2117 3737 2004 19978 12846 2049 2904 2144 2059 2085 2111 2156 2009 2008 2049 3244 2000 3244 2007 1996 2060 2307 26051 1996 2060 2307 12846 2015 1997 1996 2088 10223 26371 26925 2054 2288 2017 4699 1999 1996 10077 2198 14043 7367 19422 2906 2092 2026 2388 2003 9508 14043 1045 3473 2039 1999 4203 10768 2047 3290 2021 2026 2269 2001 1999 1996 2250 2486 2061 2057 2973 1999 3577 2005 2093 2086 2004 1037 2200 2402 2775 1045 2973 1999 2605 2021 1045 2123 2102 3342 2205 2172 1997 2008 2002 2001 2036 8895 1999 11932 1999 6646 2021 21981 2467 2175 2067 2000 4203 10768 2073 2026 6696 2155 2020 2013 2467 2175 2067 2000 1996 2155 24050 2015 1996 11938 4234 1998 15060 4596 2073 2017 2052 2031 20238 2119 2417 1998 2665 2007 2115 4977 2017 2094 2031 22136 2015 1998 13433 19454 2063 9781 2015 20717 2015 2007 2115 4977 2061 1045 2018 2023 4434 2000 2108 7402 2021 2138 1045 2018 2023 4281 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3763 2833 10223 26371 26925 2053 2198 14043 7367 19422 2906 2027 2134 2102 3305 2055 4916 2137 2833 2027 2387 2009 2004 1037 4942 2504 2117 3737 2004 19978 12846 2049 2904 2144 2059 2085 2111 2156 2009 2008 2049 3244 2000 3244 2007 1996 2060 2307 26051 1996 2060 2307 12846 2015 1997 1996 2088 10223 26371 26925 2054 2288 2017 4699 1999 1996 10077 2198 14043 7367 19422 2906 2092 2026 2388 2003 9508 14043 1045 3473 2039 1999 4203 10768 2047 3290 2021 2026 2269 2001 1999 1996 2250 2486 2061 2057 2973 1999 3577 2005 2093 2086 2004 1037 2200 2402 2775 1045 2973 1999 2605 2021 1045 2123 2102 3342 2205 2172 1997 2008 2002 2001 2036 8895 1999 11932 1999 6646 2021 21981 2467 2175 2067 2000 4203 10768 2073 2026 6696 2155 2020 2013 2467 2175 2067 2000 1996 2155 24050 2015 1996 11938 4234 1998 15060 4596 2073 2017 2052 2031 20238 2119 2417 1998 2665 2007 2115 4977 2017 2094 2031 22136 2015 1998 13433 19454 2063 9781 2015 20717 2015 2007 2115 4977 2061 1045 2018 2023 4434 2000 2108 7402 2021 2138 1045 2018 2023 4281 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 3 (id = 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 3 (id = 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] chili both red and green with your turkey you ##d have chico ##s and po ##sol ##e corn ##s stew ##s with your turkey so i had this connection to being latino but because i had this background of extensive travel everything was a mixture to me everything was latino as an anchor but then there was this layer of okay what ##s the rest of the world doing how are we going to filter my experience my identity through latino eyes and this fusion of the other things in the world lou ##rdes campos was your mom a cook at the time or did you learn a lot from your mother john rivera se ##dl ##ar not about cooking my mother was a pretty good cook 00 05 00 even though we ja ##b her little bit that she was a modern 50 ##s cook meaning that it was the style of the times to be a modern cook what did that mean that mean frozen green beans canned peas tv dinners onion soup with sour cream and dip so she was a thoroughly modern cook and then the world rebelled the advent of the farmers markets [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] chili both red and green with your turkey you ##d have chico ##s and po ##sol ##e corn ##s stew ##s with your turkey so i had this connection to being latino but because i had this background of extensive travel everything was a mixture to me everything was latino as an anchor but then there was this layer of okay what ##s the rest of the world doing how are we going to filter my experience my identity through latino eyes and this fusion of the other things in the world lou ##rdes campos was your mom a cook at the time or did you learn a lot from your mother john rivera se ##dl ##ar not about cooking my mother was a pretty good cook 00 05 00 even though we ja ##b her little bit that she was a modern 50 ##s cook meaning that it was the style of the times to be a modern cook what did that mean that mean frozen green beans canned peas tv dinners onion soup with sour cream and dip so she was a thoroughly modern cook and then the world rebelled the advent of the farmers markets [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 20238 2119 2417 1998 2665 2007 2115 4977 2017 2094 2031 22136 2015 1998 13433 19454 2063 9781 2015 20717 2015 2007 2115 4977 2061 1045 2018 2023 4434 2000 2108 7402 2021 2138 1045 2018 2023 4281 1997 4866 3604 2673 2001 1037 8150 2000 2033 2673 2001 7402 2004 2019 8133 2021 2059 2045 2001 2023 6741 1997 3100 2054 2015 1996 2717 1997 1996 2088 2725 2129 2024 2057 2183 2000 11307 2026 3325 2026 4767 2083 7402 2159 1998 2023 10077 1997 1996 2060 2477 1999 1996 2088 10223 26371 26925 2001 2115 3566 1037 5660 2012 1996 2051 2030 2106 2017 4553 1037 2843 2013 2115 2388 2198 14043 7367 19422 2906 2025 2055 8434 2026 2388 2001 1037 3492 2204 5660 4002 5709 4002 2130 2295 2057 14855 2497 2014 2210 2978 2008 2016 2001 1037 2715 2753 2015 5660 3574 2008 2009 2001 1996 2806 1997 1996 2335 2000 2022 1037 2715 5660 2054 2106 2008 2812 2008 2812 7708 2665 13435 27141 26072 2694 29236 20949 11350 2007 14768 6949 1998 16510 2061 2016 2001 1037 12246 2715 5660 1998 2059 1996 2088 25183 1996 13896 1997 1996 6617 6089 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 20238 2119 2417 1998 2665 2007 2115 4977 2017 2094 2031 22136 2015 1998 13433 19454 2063 9781 2015 20717 2015 2007 2115 4977 2061 1045 2018 2023 4434 2000 2108 7402 2021 2138 1045 2018 2023 4281 1997 4866 3604 2673 2001 1037 8150 2000 2033 2673 2001 7402 2004 2019 8133 2021 2059 2045 2001 2023 6741 1997 3100 2054 2015 1996 2717 1997 1996 2088 2725 2129 2024 2057 2183 2000 11307 2026 3325 2026 4767 2083 7402 2159 1998 2023 10077 1997 1996 2060 2477 1999 1996 2088 10223 26371 26925 2001 2115 3566 1037 5660 2012 1996 2051 2030 2106 2017 4553 1037 2843 2013 2115 2388 2198 14043 7367 19422 2906 2025 2055 8434 2026 2388 2001 1037 3492 2204 5660 4002 5709 4002 2130 2295 2057 14855 2497 2014 2210 2978 2008 2016 2001 1037 2715 2753 2015 5660 3574 2008 2009 2001 1996 2806 1997 1996 2335 2000 2022 1037 2715 5660 2054 2106 2008 2812 2008 2812 7708 2665 13435 27141 26072 2694 29236 20949 11350 2007 14768 6949 1998 16510 2061 2016 2001 1037 12246 2715 5660 1998 2059 1996 2088 25183 1996 13896 1997 1996 6617 6089 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 3 (id = 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 3 (id = 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] barbara ash ##ud 00 00 00 my name is bar ##bra ash ##ud im 69 years old today is april the 2nd 2011 were in long beach california and im a friend of bertha bertha o ##cal my name is bertha o ##cal i am 28 years old today is april 2nd 2011 were in long beach california and bar ##bra is a friend of mine female _ 1 so bertha what made you decide to come record a conversation today bertha o ##cal every life has a story to tell and i am so blessed that iv ##e had so many experiences in my life this is a wonderful opportunity to be able to not only share with the world but be able to give something back to my brothers and sisters and know that everything is possible female _ 1 okay so where are you from bertha o ##cal i was originally born in honduras central america i was actually born december 28 1981 my mom always says its the day of the innocent female _ 1 what does that mean bertha o ##cal well its the day of a saint i thought well wow i [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] barbara ash ##ud 00 00 00 my name is bar ##bra ash ##ud im 69 years old today is april the 2nd 2011 were in long beach california and im a friend of bertha bertha o ##cal my name is bertha o ##cal i am 28 years old today is april 2nd 2011 were in long beach california and bar ##bra is a friend of mine female _ 1 so bertha what made you decide to come record a conversation today bertha o ##cal every life has a story to tell and i am so blessed that iv ##e had so many experiences in my life this is a wonderful opportunity to be able to not only share with the world but be able to give something back to my brothers and sisters and know that everything is possible female _ 1 okay so where are you from bertha o ##cal i was originally born in honduras central america i was actually born december 28 1981 my mom always says its the day of the innocent female _ 1 what does that mean bertha o ##cal well its the day of a saint i thought well wow i [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 6437 6683 6784 4002 4002 4002 2026 2171 2003 3347 10024 6683 6784 10047 6353 2086 2214 2651 2003 2258 1996 3416 2249 2020 1999 2146 3509 2662 1998 10047 1037 2767 1997 25079 25079 1051 9289 2026 2171 2003 25079 1051 9289 1045 2572 2654 2086 2214 2651 2003 2258 3416 2249 2020 1999 2146 3509 2662 1998 3347 10024 2003 1037 2767 1997 3067 2931 1035 1015 2061 25079 2054 2081 2017 5630 2000 2272 2501 1037 4512 2651 25079 1051 9289 2296 2166 2038 1037 2466 2000 2425 1998 1045 2572 2061 10190 2008 4921 2063 2018 2061 2116 6322 1999 2026 2166 2023 2003 1037 6919 4495 2000 2022 2583 2000 2025 2069 3745 2007 1996 2088 2021 2022 2583 2000 2507 2242 2067 2000 2026 3428 1998 5208 1998 2113 2008 2673 2003 2825 2931 1035 1015 3100 2061 2073 2024 2017 2013 25079 1051 9289 1045 2001 2761 2141 1999 14373 2430 2637 1045 2001 2941 2141 2285 2654 3261 2026 3566 2467 2758 2049 1996 2154 1997 1996 7036 2931 1035 1015 2054 2515 2008 2812 25079 1051 9289 2092 2049 1996 2154 1997 1037 3002 1045 2245 2092 10166 1045 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 6437 6683 6784 4002 4002 4002 2026 2171 2003 3347 10024 6683 6784 10047 6353 2086 2214 2651 2003 2258 1996 3416 2249 2020 1999 2146 3509 2662 1998 10047 1037 2767 1997 25079 25079 1051 9289 2026 2171 2003 25079 1051 9289 1045 2572 2654 2086 2214 2651 2003 2258 3416 2249 2020 1999 2146 3509 2662 1998 3347 10024 2003 1037 2767 1997 3067 2931 1035 1015 2061 25079 2054 2081 2017 5630 2000 2272 2501 1037 4512 2651 25079 1051 9289 2296 2166 2038 1037 2466 2000 2425 1998 1045 2572 2061 10190 2008 4921 2063 2018 2061 2116 6322 1999 2026 2166 2023 2003 1037 6919 4495 2000 2022 2583 2000 2025 2069 3745 2007 1996 2088 2021 2022 2583 2000 2507 2242 2067 2000 2026 3428 1998 5208 1998 2113 2008 2673 2003 2825 2931 1035 1015 3100 2061 2073 2024 2017 2013 25079 1051 9289 1045 2001 2761 2141 1999 14373 2430 2637 1045 2001 2941 2141 2285 2654 3261 2026 3566 2467 2758 2049 1996 2154 1997 1996 7036 2931 1035 1015 2054 2515 2008 2812 25079 1051 9289 2092 2049 1996 2154 1997 1037 3002 1045 2245 2092 10166 1045 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] december 28 1981 my mom always says its the day of the innocent female _ 1 what does that mean bertha o ##cal well its the day of a saint i thought well wow i have a lot of pressure to be really good but i moved out to the united states with my entire family we migrated november 21st 1989 since then iv ##e never looked back and just continue to move forward female _ 1 so what do you remember about where you were born bertha o ##cal i appreciate the everyday struggles that we had in honduras because they were not the same we take things for granted here in the us sometimes it was as small as maybe not having warm water to take a shower or not having a wash ##er and dry ##er and as kids we didn ##t really have toys my parents couldn ##t afford to buy toys so we used our imagination quite a bit sometimes when i see little kids and they have all these little toys and im thinking wow if you could just play make belief and how fun that would be that you can take [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] december 28 1981 my mom always says its the day of the innocent female _ 1 what does that mean bertha o ##cal well its the day of a saint i thought well wow i have a lot of pressure to be really good but i moved out to the united states with my entire family we migrated november 21st 1989 since then iv ##e never looked back and just continue to move forward female _ 1 so what do you remember about where you were born bertha o ##cal i appreciate the everyday struggles that we had in honduras because they were not the same we take things for granted here in the us sometimes it was as small as maybe not having warm water to take a shower or not having a wash ##er and dry ##er and as kids we didn ##t really have toys my parents couldn ##t afford to buy toys so we used our imagination quite a bit sometimes when i see little kids and they have all these little toys and im thinking wow if you could just play make belief and how fun that would be that you can take [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2285 2654 3261 2026 3566 2467 2758 2049 1996 2154 1997 1996 7036 2931 1035 1015 2054 2515 2008 2812 25079 1051 9289 2092 2049 1996 2154 1997 1037 3002 1045 2245 2092 10166 1045 2031 1037 2843 1997 3778 2000 2022 2428 2204 2021 1045 2333 2041 2000 1996 2142 2163 2007 2026 2972 2155 2057 13447 2281 7398 2960 2144 2059 4921 2063 2196 2246 2067 1998 2074 3613 2000 2693 2830 2931 1035 1015 2061 2054 2079 2017 3342 2055 2073 2017 2020 2141 25079 1051 9289 1045 9120 1996 10126 11785 2008 2057 2018 1999 14373 2138 2027 2020 2025 1996 2168 2057 2202 2477 2005 4379 2182 1999 1996 2149 2823 2009 2001 2004 2235 2004 2672 2025 2383 4010 2300 2000 2202 1037 6457 2030 2025 2383 1037 9378 2121 1998 4318 2121 1998 2004 4268 2057 2134 2102 2428 2031 10899 2026 3008 2481 2102 8984 2000 4965 10899 2061 2057 2109 2256 9647 3243 1037 2978 2823 2043 1045 2156 2210 4268 1998 2027 2031 2035 2122 2210 10899 1998 10047 3241 10166 2065 2017 2071 2074 2377 2191 6772 1998 2129 4569 2008 2052 2022 2008 2017 2064 2202 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2285 2654 3261 2026 3566 2467 2758 2049 1996 2154 1997 1996 7036 2931 1035 1015 2054 2515 2008 2812 25079 1051 9289 2092 2049 1996 2154 1997 1037 3002 1045 2245 2092 10166 1045 2031 1037 2843 1997 3778 2000 2022 2428 2204 2021 1045 2333 2041 2000 1996 2142 2163 2007 2026 2972 2155 2057 13447 2281 7398 2960 2144 2059 4921 2063 2196 2246 2067 1998 2074 3613 2000 2693 2830 2931 1035 1015 2061 2054 2079 2017 3342 2055 2073 2017 2020 2141 25079 1051 9289 1045 9120 1996 10126 11785 2008 2057 2018 1999 14373 2138 2027 2020 2025 1996 2168 2057 2202 2477 2005 4379 2182 1999 1996 2149 2823 2009 2001 2004 2235 2004 2672 2025 2383 4010 2300 2000 2202 1037 6457 2030 2025 2383 1037 9378 2121 1998 4318 2121 1998 2004 4268 2057 2134 2102 2428 2031 10899 2026 3008 2481 2102 8984 2000 4965 10899 2061 2057 2109 2256 9647 3243 1037 2978 2823 2043 1045 2156 2210 4268 1998 2027 2031 2035 2122 2210 10899 1998 10047 3241 10166 2065 2017 2071 2074 2377 2191 6772 1998 2129 4569 2008 2052 2022 2008 2017 2064 2202 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] quite a bit sometimes when i see little kids and they have all these little toys and im thinking wow if you could just play make belief and how fun that would be that you can take a box and turn it into like a house or take the leaves from a plant and use that and cut into a little circle and that ##s a little tor ##till ##a because your ##e playing house so there ##s little things like that that i appreciate now as an adult and even though we did struggle i think that its allowed me to become the woman that i am now female _ 1 what do you know about how your family got from honduras to the united states bertha o ##cal its always the us always represented for my family a fresh new start an opportunity to give back they always wanted to give us more and they wanted us to be happy and safe and the us represented that for them a long time ago my grandmother migrated to the us she was an illegal undo ##cum ##ented immigrant better said sorry but she struggled out here and [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] quite a bit sometimes when i see little kids and they have all these little toys and im thinking wow if you could just play make belief and how fun that would be that you can take a box and turn it into like a house or take the leaves from a plant and use that and cut into a little circle and that ##s a little tor ##till ##a because your ##e playing house so there ##s little things like that that i appreciate now as an adult and even though we did struggle i think that its allowed me to become the woman that i am now female _ 1 what do you know about how your family got from honduras to the united states bertha o ##cal its always the us always represented for my family a fresh new start an opportunity to give back they always wanted to give us more and they wanted us to be happy and safe and the us represented that for them a long time ago my grandmother migrated to the us she was an illegal undo ##cum ##ented immigrant better said sorry but she struggled out here and [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3243 1037 2978 2823 2043 1045 2156 2210 4268 1998 2027 2031 2035 2122 2210 10899 1998 10047 3241 10166 2065 2017 2071 2074 2377 2191 6772 1998 2129 4569 2008 2052 2022 2008 2017 2064 2202 1037 3482 1998 2735 2009 2046 2066 1037 2160 2030 2202 1996 3727 2013 1037 3269 1998 2224 2008 1998 3013 2046 1037 2210 4418 1998 2008 2015 1037 2210 17153 28345 2050 2138 2115 2063 2652 2160 2061 2045 2015 2210 2477 2066 2008 2008 1045 9120 2085 2004 2019 4639 1998 2130 2295 2057 2106 5998 1045 2228 2008 2049 3039 2033 2000 2468 1996 2450 2008 1045 2572 2085 2931 1035 1015 2054 2079 2017 2113 2055 2129 2115 2155 2288 2013 14373 2000 1996 2142 2163 25079 1051 9289 2049 2467 1996 2149 2467 3421 2005 2026 2155 1037 4840 2047 2707 2019 4495 2000 2507 2067 2027 2467 2359 2000 2507 2149 2062 1998 2027 2359 2149 2000 2022 3407 1998 3647 1998 1996 2149 3421 2008 2005 2068 1037 2146 2051 3283 2026 7133 13447 2000 1996 2149 2016 2001 2019 6206 25672 24894 14088 11560 2488 2056 3374 2021 2016 6915 2041 2182 1998 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3243 1037 2978 2823 2043 1045 2156 2210 4268 1998 2027 2031 2035 2122 2210 10899 1998 10047 3241 10166 2065 2017 2071 2074 2377 2191 6772 1998 2129 4569 2008 2052 2022 2008 2017 2064 2202 1037 3482 1998 2735 2009 2046 2066 1037 2160 2030 2202 1996 3727 2013 1037 3269 1998 2224 2008 1998 3013 2046 1037 2210 4418 1998 2008 2015 1037 2210 17153 28345 2050 2138 2115 2063 2652 2160 2061 2045 2015 2210 2477 2066 2008 2008 1045 9120 2085 2004 2019 4639 1998 2130 2295 2057 2106 5998 1045 2228 2008 2049 3039 2033 2000 2468 1996 2450 2008 1045 2572 2085 2931 1035 1015 2054 2079 2017 2113 2055 2129 2115 2155 2288 2013 14373 2000 1996 2142 2163 25079 1051 9289 2049 2467 1996 2149 2467 3421 2005 2026 2155 1037 4840 2047 2707 2019 4495 2000 2507 2067 2027 2467 2359 2000 2507 2149 2062 1998 2027 2359 2149 2000 2022 3407 1998 3647 1998 1996 2149 3421 2008 2005 2068 1037 2146 2051 3283 2026 7133 13447 2000 1996 2149 2016 2001 2019 6206 25672 24894 14088 11560 2488 2056 3374 2021 2016 6915 2041 2182 1998 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] us to be happy and safe and the us represented that for them a long time ago my grandmother migrated to the us she was an illegal undo ##cum ##ented immigrant better said sorry but she struggled out here and eventually she worked really hard to gain her citizenship and through her citizenship she requested us to come out to the us my dad wouldn ##t have it any other way unless we all came as a family so we all decided that it was the right thing to do i was a little scared we didn ##t know any english and so that was in itself a big elephant because we had to learn how to speak english and get around california is so big before we walked everywhere and here we had to learn how to take the bus and learn how to communicate so we can get around so that was also an experience in its own female _ 1 how old were you then bertha o ##cal i came to the us in 89 so i was eight years old and my birthday ##s on the 28 of december so i was eight going [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] us to be happy and safe and the us represented that for them a long time ago my grandmother migrated to the us she was an illegal undo ##cum ##ented immigrant better said sorry but she struggled out here and eventually she worked really hard to gain her citizenship and through her citizenship she requested us to come out to the us my dad wouldn ##t have it any other way unless we all came as a family so we all decided that it was the right thing to do i was a little scared we didn ##t know any english and so that was in itself a big elephant because we had to learn how to speak english and get around california is so big before we walked everywhere and here we had to learn how to take the bus and learn how to communicate so we can get around so that was also an experience in its own female _ 1 how old were you then bertha o ##cal i came to the us in 89 so i was eight years old and my birthday ##s on the 28 of december so i was eight going [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2149 2000 2022 3407 1998 3647 1998 1996 2149 3421 2008 2005 2068 1037 2146 2051 3283 2026 7133 13447 2000 1996 2149 2016 2001 2019 6206 25672 24894 14088 11560 2488 2056 3374 2021 2016 6915 2041 2182 1998 2776 2016 2499 2428 2524 2000 5114 2014 9068 1998 2083 2014 9068 2016 7303 2149 2000 2272 2041 2000 1996 2149 2026 3611 2876 2102 2031 2009 2151 2060 2126 4983 2057 2035 2234 2004 1037 2155 2061 2057 2035 2787 2008 2009 2001 1996 2157 2518 2000 2079 1045 2001 1037 2210 6015 2057 2134 2102 2113 2151 2394 1998 2061 2008 2001 1999 2993 1037 2502 10777 2138 2057 2018 2000 4553 2129 2000 3713 2394 1998 2131 2105 2662 2003 2061 2502 2077 2057 2939 7249 1998 2182 2057 2018 2000 4553 2129 2000 2202 1996 3902 1998 4553 2129 2000 10639 2061 2057 2064 2131 2105 2061 2008 2001 2036 2019 3325 1999 2049 2219 2931 1035 1015 2129 2214 2020 2017 2059 25079 1051 9289 1045 2234 2000 1996 2149 1999 6486 2061 1045 2001 2809 2086 2214 1998 2026 5798 2015 2006 1996 2654 1997 2285 2061 1045 2001 2809 2183 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2149 2000 2022 3407 1998 3647 1998 1996 2149 3421 2008 2005 2068 1037 2146 2051 3283 2026 7133 13447 2000 1996 2149 2016 2001 2019 6206 25672 24894 14088 11560 2488 2056 3374 2021 2016 6915 2041 2182 1998 2776 2016 2499 2428 2524 2000 5114 2014 9068 1998 2083 2014 9068 2016 7303 2149 2000 2272 2041 2000 1996 2149 2026 3611 2876 2102 2031 2009 2151 2060 2126 4983 2057 2035 2234 2004 1037 2155 2061 2057 2035 2787 2008 2009 2001 1996 2157 2518 2000 2079 1045 2001 1037 2210 6015 2057 2134 2102 2113 2151 2394 1998 2061 2008 2001 1999 2993 1037 2502 10777 2138 2057 2018 2000 4553 2129 2000 3713 2394 1998 2131 2105 2662 2003 2061 2502 2077 2057 2939 7249 1998 2182 2057 2018 2000 4553 2129 2000 2202 1996 3902 1998 4553 2129 2000 10639 2061 2057 2064 2131 2105 2061 2008 2001 2036 2019 3325 1999 2049 2219 2931 1035 1015 2129 2214 2020 2017 2059 25079 1051 9289 1045 2234 2000 1996 2149 1999 6486 2061 1045 2001 2809 2086 2214 1998 2026 5798 2015 2006 1996 2654 1997 2285 2061 1045 2001 2809 2183 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] also an experience in its own female _ 1 how old were you then bertha o ##cal i came to the us in 89 so i was eight years old and my birthday ##s on the 28 of december so i was eight going on nine years old female _ 1 and bertha what kind of kid were you at eight years old bertha o ##cal wow well i was always the one that got in trouble i had a beautiful life in honduras despite the struggles like i said i used my imagination a lot so we played pretend make belief i was either steward ##ess or a banker i was a doctor i was a teacher and i had friends next door i think i was just a little kid that always had that imagination and was determined that if it was raining i was still going to have a good day and we were going to remain positive and have a good time but i did get into a lot of trouble because of that because i 00 05 00 always found a way to keep things interesting at home but my mom knew that [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] also an experience in its own female _ 1 how old were you then bertha o ##cal i came to the us in 89 so i was eight years old and my birthday ##s on the 28 of december so i was eight going on nine years old female _ 1 and bertha what kind of kid were you at eight years old bertha o ##cal wow well i was always the one that got in trouble i had a beautiful life in honduras despite the struggles like i said i used my imagination a lot so we played pretend make belief i was either steward ##ess or a banker i was a doctor i was a teacher and i had friends next door i think i was just a little kid that always had that imagination and was determined that if it was raining i was still going to have a good day and we were going to remain positive and have a good time but i did get into a lot of trouble because of that because i 00 05 00 always found a way to keep things interesting at home but my mom knew that [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2036 2019 3325 1999 2049 2219 2931 1035 1015 2129 2214 2020 2017 2059 25079 1051 9289 1045 2234 2000 1996 2149 1999 6486 2061 1045 2001 2809 2086 2214 1998 2026 5798 2015 2006 1996 2654 1997 2285 2061 1045 2001 2809 2183 2006 3157 2086 2214 2931 1035 1015 1998 25079 2054 2785 1997 4845 2020 2017 2012 2809 2086 2214 25079 1051 9289 10166 2092 1045 2001 2467 1996 2028 2008 2288 1999 4390 1045 2018 1037 3376 2166 1999 14373 2750 1996 11785 2066 1045 2056 1045 2109 2026 9647 1037 2843 2061 2057 2209 9811 2191 6772 1045 2001 2593 17946 7971 2030 1037 13448 1045 2001 1037 3460 1045 2001 1037 3836 1998 1045 2018 2814 2279 2341 1045 2228 1045 2001 2074 1037 2210 4845 2008 2467 2018 2008 9647 1998 2001 4340 2008 2065 2009 2001 24057 1045 2001 2145 2183 2000 2031 1037 2204 2154 1998 2057 2020 2183 2000 3961 3893 1998 2031 1037 2204 2051 2021 1045 2106 2131 2046 1037 2843 1997 4390 2138 1997 2008 2138 1045 4002 5709 4002 2467 2179 1037 2126 2000 2562 2477 5875 2012 2188 2021 2026 3566 2354 2008 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2036 2019 3325 1999 2049 2219 2931 1035 1015 2129 2214 2020 2017 2059 25079 1051 9289 1045 2234 2000 1996 2149 1999 6486 2061 1045 2001 2809 2086 2214 1998 2026 5798 2015 2006 1996 2654 1997 2285 2061 1045 2001 2809 2183 2006 3157 2086 2214 2931 1035 1015 1998 25079 2054 2785 1997 4845 2020 2017 2012 2809 2086 2214 25079 1051 9289 10166 2092 1045 2001 2467 1996 2028 2008 2288 1999 4390 1045 2018 1037 3376 2166 1999 14373 2750 1996 11785 2066 1045 2056 1045 2109 2026 9647 1037 2843 2061 2057 2209 9811 2191 6772 1045 2001 2593 17946 7971 2030 1037 13448 1045 2001 1037 3460 1045 2001 1037 3836 1998 1045 2018 2814 2279 2341 1045 2228 1045 2001 2074 1037 2210 4845 2008 2467 2018 2008 9647 1998 2001 4340 2008 2065 2009 2001 24057 1045 2001 2145 2183 2000 2031 1037 2204 2154 1998 2057 2020 2183 2000 3961 3893 1998 2031 1037 2204 2051 2021 1045 2106 2131 2046 1037 2843 1997 4390 2138 1997 2008 2138 1045 4002 5709 4002 2467 2179 1037 2126 2000 2562 2477 5875 2012 2188 2021 2026 3566 2354 2008 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert our train and validation features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "val_features = bert.run_classifier.convert_examples_to_features(val_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "  # Use \"sequence_outputs\" for token-level output.\n",
    "  output_layer = bert_outputs[\"pooled_output\"]\n",
    "  # with tf.Session() as sess:\n",
    "  output_layer1 = bert_outputs[\"pooled_output\"]\n",
    "  # output_layer1 = 999\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "  # Create our own layer to tune for politeness data.\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # Dropout helps prevent overfitting\n",
    "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.8)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "      return (predicted_labels, log_probs, output_layer1)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        true_pos = tf.metrics.true_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        true_neg = tf.metrics.true_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)   \n",
    "        false_pos = tf.metrics.false_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)  \n",
    "        false_neg = tf.metrics.false_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        \n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"true_positives\": true_pos,\n",
    "            \"true_negatives\": true_neg,\n",
    "            \"false_positives\": false_pos,\n",
    "            \"false_negatives\": false_neg,\n",
    "            }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs, output_layer) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels,\n",
    "          'pooled_output': output_layer\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 1.0\n",
    "# Warmup is a period of time where the learning rate is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 300\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "\n",
    "# Compute train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "# Specify output directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
    "\n",
    "# Specify output directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171, 4)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_steps, len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './bert_output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 300, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002852D9D5898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './bert_output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 300, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002852D9D5898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "#Initializing the model and the estimator\n",
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)\n",
    "\n",
    "# Create an input function for validating. drop_remainder = True for using TPUs.\n",
    "val_input_fn = run_classifier.input_fn_builder(\n",
    "    features=val_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./bert_output/model.ckpt-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./bert_output/model.ckpt-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into ./bert_output/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into ./bert_output/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.2994007, step = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.2994007, step = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 2.4731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 2.4731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.8742725, step = 100 (40.436 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.8742725, step = 100 (40.436 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 171 into ./bert_output/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 171 into ./bert_output/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 2.261801.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 2.261801.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took time  0:02:22.544815\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "C:\\Users\\ej\\anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2020-09-19T03:16:54Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2020-09-19T03:16:54Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./bert_output/model.ckpt-171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./bert_output/model.ckpt-171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2020-09-19-03:17:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2020-09-19-03:17:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 171: eval_accuracy = 0.92918456, false_negatives = 0.0, false_positives = 0.0, global_step = 171, loss = 0.32039884, true_negatives = 0.0, true_positives = 466.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 171: eval_accuracy = 0.92918456, false_negatives = 0.0, false_positives = 0.0, global_step = 171, loss = 0.32039884, true_negatives = 0.0, true_positives = 466.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 171: ./bert_output/model.ckpt-171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 171: ./bert_output/model.ckpt-171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_accuracy': 0.92918456,\n",
       " 'false_negatives': 0.0,\n",
       " 'false_positives': 0.0,\n",
       " 'loss': 0.32039884,\n",
       " 'true_negatives': 0.0,\n",
       " 'true_positives': 466.0,\n",
       " 'global_step': 171}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating the model with Validation set\n",
    "estimator.evaluate(input_fn=val_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to get predictions\n",
    "def getPrediction(in_sentences, type_output = \"features\"):\n",
    "  #A list to map the actual labels to the predictions\n",
    "  labels = np.unique(train['label'])\n",
    "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] \n",
    "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "  #Predicting the classes \n",
    "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "  predictions = estimator.predict(predict_input_fn)\n",
    "  if type_output == \"features\":\n",
    "    return [prediction['pooled_output'] for _,prediction in enumerate(predictions) ]\n",
    "  else:\n",
    "    return ([(sentence, prediction['probabilities'],\n",
    "              prediction['labels'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.logging.ERROR)\n",
    "MAX_SEQ_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1715, 2), (466, 2))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tr_emb = np.apply_along_axis(getPrediction, 0,np.array(train_df[DATA_COLUMN]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(466, 768)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val_emb = np.apply_along_axis(getPrediction, 0,np.array(val_df[DATA_COLUMN]))\n",
    "val_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((466, 768), (1715, 768))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_emb.shape, tr_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux = -1\n",
    "len_l = 0\n",
    "train_x = {}\n",
    "for l, emb in zip(index_l, tr_emb):\n",
    "  if l in train_x.keys():\n",
    "    train_x[l]  =np.vstack([train_x[l], emb])\n",
    "  else:\n",
    "    train_x[l] = [emb]\n",
    "\n",
    "len(train_x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.46535116, 0.080982454, 0.8267497, 0.407189...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.45397788, 0.07643733, 0.8317803, 0.4139836...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.47631255, 0.08443489, 0.8220827, 0.4003446...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.46101007, 0.0808071, 0.82787544, 0.4112950...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.47833195, 0.08284731, 0.82518005, 0.398153...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 emb  label\n",
       "0  [[0.46535116, 0.080982454, 0.8267497, 0.407189...      3\n",
       "1  [[0.45397788, 0.07643733, 0.8317803, 0.4139836...      3\n",
       "2  [[0.47631255, 0.08443489, 0.8220827, 0.4003446...      1\n",
       "3  [[0.46101007, 0.0808071, 0.82787544, 0.4112950...      1\n",
       "4  [[0.47833195, 0.08284731, 0.82518005, 0.398153...      1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_l_final = []\n",
    "label_l_final = []\n",
    "for k in train_x.keys():\n",
    "  train_l_final.append(train_x[k])\n",
    "  label_l_final.append(train.loc[k]['label'])\n",
    "\n",
    "df_train = pd.DataFrame({'emb': train_l_final, 'label': label_l_final, })\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.4751098, 0.08600618, 0.82669735, 0.3994270...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.45415825, 0.07889505, 0.8322021, 0.4159758...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.47881222, 0.08399956, 0.8227745, 0.3997500...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.46033302, 0.083140366, 0.828532, 0.4075922...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.46017715, 0.08624659, 0.8292396, 0.4078793...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 emb  label\n",
       "0  [[0.4751098, 0.08600618, 0.82669735, 0.3994270...      1\n",
       "1  [[0.45415825, 0.07889505, 0.8322021, 0.4159758...      1\n",
       "2  [[0.47881222, 0.08399956, 0.8227745, 0.3997500...      1\n",
       "3  [[0.46033302, 0.083140366, 0.828532, 0.4075922...      1\n",
       "4  [[0.46017715, 0.08624659, 0.8292396, 0.4078793...      1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux = -1\n",
    "len_l = 0\n",
    "val_x = {}\n",
    "\n",
    "for l, emb in zip(val_index_l, val_emb):\n",
    "  if l in val_x.keys():\n",
    "    val_x[l]  =np.vstack([val_x[l], emb])\n",
    "  else:\n",
    "    val_x[l] = [emb]\n",
    "\n",
    "\n",
    "val_l_final = []\n",
    "vlabel_l_final = []\n",
    "for k in val_x.keys():\n",
    "  val_l_final.append(val_x[k])\n",
    "  vlabel_l_final.append(val.loc[k]['label'])\n",
    "\n",
    "df_val = pd.DataFrame({'emb': val_l_final, 'label': vlabel_l_final})\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val, df_test = train_test_split(df_val, test_size=0.4, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            (None, None, 768)         0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, None, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               347600    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 124       \n",
      "=================================================================\n",
      "Total params: 350,754\n",
      "Trainable params: 350,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "text_input = Input(shape=(None,768,), dtype='float32', name='text')\n",
    "\n",
    "l_mask = layers.Masking(mask_value=-99.)(text_input)\n",
    "# Which we encoded in a single vector via a LSTM\n",
    "encoded_text = layers.LSTM(100,)(l_mask)\n",
    "out_dense = layers.Dense(30, activation='relu')(encoded_text)\n",
    "# And we add a softmax classifier on top\n",
    "out = layers.Dense(len(label_list), activation='softmax')(out_dense)\n",
    "# At model instantiation, we specify the input and the output:\n",
    "model = Model(text_input, out)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45, 2), (7, 2), (5, 2))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences = len(df_train['emb'].to_list())\n",
    "batch_size = 3\n",
    "batches_per_epoch =  15\n",
    "assert batch_size * batches_per_epoch == num_sequences\n",
    "num_features= 768\n",
    "def train_generator(df):\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch):\n",
    "            longest_index = (b + 1) * batch_size - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size][-batch_size:], key=len))\n",
    "            x_train = np.full((batch_size, timesteps, num_features), -99.)\n",
    "            y_train = np.zeros((batch_size,  1))\n",
    "            for i in range(batch_size):\n",
    "                li = b * batch_size + i\n",
    "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_train[i] = y_list[li]\n",
    "            yield x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences_val = len(df_val['emb'].to_list())\n",
    "batch_size_val = 1\n",
    "batches_per_epoch_val = 7\n",
    "assert batch_size_val * batches_per_epoch_val == num_sequences_val\n",
    "num_features= 768\n",
    "def val_generator(df):\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_val):\n",
    "            longest_index = (b + 1) * batch_size_val - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_val][-31:], key=len))\n",
    "            # print(len(df_train['emb'].to_list()[:b+batch_size][-7:]))\n",
    "            x_train = np.full((batch_size_val, timesteps, num_features), -99.)\n",
    "            y_train = np.zeros((batch_size_val,  1))\n",
    "            for i in range(batch_size_val):\n",
    "                li = b * batch_size_val + i\n",
    "                # print(\"li\", li)\n",
    "                # print(x_train[i, 0:len(x_list[li]), :].shape, len(x_list[li]))\n",
    "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_train[i] = y_list[li]\n",
    "            yield x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "call_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 1s 55ms/step - loss: 1.0651 - acc: 0.5111 - val_loss: 1.5421 - val_acc: 0.8571\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.6554 - acc: 0.8000 - val_loss: 1.9506 - val_acc: 0.8571\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.6440 - acc: 0.8000 - val_loss: 1.9287 - val_acc: 0.8571\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.6410 - acc: 0.8000 - val_loss: 1.8397 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.6381 - acc: 0.8000 - val_loss: 1.8153 - val_acc: 0.8571\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.6372 - acc: 0.8000 - val_loss: 1.8212 - val_acc: 0.8571\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.6369 - acc: 0.8000 - val_loss: 1.8292 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.6366 - acc: 0.8000 - val_loss: 1.8323 - val_acc: 0.8571\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.6365 - acc: 0.8000 - val_loss: 1.8329 - val_acc: 0.8571\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.6364 - acc: 0.8000 - val_loss: 1.8331 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x286ebe162e8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator(df_train), steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_generator(df_val), validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2293705940246582, 1.0]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sequences_val = len(df_test['emb'].to_list())\n",
    "batch_size_val = 1\n",
    "batches_per_epoch_val = 5\n",
    "assert batch_size_val * batches_per_epoch_val == num_sequences_val\n",
    "num_features= 768\n",
    "model.evaluate_generator(val_generator(df_test), steps= batches_per_epoch_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
